---
title: "The Gaussian Neural Process"
collection: publications
permalink: /publication/gnp
excerpt: "We present a new member of the Neural Process family that meta-learns a map from observed datasets to posterior Gaussian processes."
date: 2021-01-10
venue: 'Advances in Approximate Bayesian Inference (AABI)'
paperurl: 'https://arxiv.org/abs/2101.03606'
citation:
---

Wessel P. Bruinsma, James Requeima, **Andrew Y. K. Foong**, Jonathan Gordon, and Richard E. Turner.

## Abstract
Neural Processes (NPs; Garnelo et al., 2018a,b) are a rich class of models for meta-learning that map data sets directly to predictive stochastic processes. We provide a rigorous analysis of the standard maximum-likelihood objective used to train conditional NPs. Moreover, we propose a new member to the Neural Process family called the Gaussian Neural Process (GNP), which models predictive correlations, incorporates translation equivariance, provides universal approximation guarantees, and demonstrates encouraging performance.
