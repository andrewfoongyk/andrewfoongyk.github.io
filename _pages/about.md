---
permalink: /
title: null #"About Me"
excerpt: "About me"
author_profile: true
redirect_from:
  - /about/
  - /about.html
---

<style>
  .page__title {
    display: none;
  }
</style>

<a id="about" style="display: block; position: relative; top: -200px; visibility: hidden;"></a>
<!-- <h1 style = "margin-top: -1.5rem;">About Me</h1> -->
<h1 class="about-heading">About Me</h1>

I am an
<span class="small-caps">ai</span>
scientist at the 
Mayo Clinic,
<!-- <a href="https://www.mayoclinic.org">Mayo Clinic</a>,  -->
where I lead the
<span class="small-caps">ai</span>
and Data Analytics 
<span class="small-caps">(aida)</span>
team in the 
Department of Radiation Oncology.
<!-- [Department of Radiation Oncology](https://www.mayoclinic.org/departments-centers/radiation-oncology/home/orc-20188588). -->
We develop next-generation 
<span class="small-caps">ai</span>
systems to advance cancer treatment.
See my [faculty page](https://www.mayo.edu/research/faculty/foong-andrew-ph-d/bio-20583559) for research areas.

Before joining Mayo, I was a senior researcher
at
Microsoft Research in Cambridge,
<!-- <a href= "https://www.microsoft.com/en-us/research/lab/microsoft-research-cambridge/">Microsoft Research</a> in Cambridge, -->
<span class="small-caps">uk</span>,
where I developed generative deep learning models for proteins in the 
<span class="small-caps">ai</span> for Science team.
<!-- [<span class="small-caps">ai</span> for Science team](https://www.microsoft.com/en-us/research/lab/microsoft-research-ai4science/). -->
Our work on
<a href="#bioemu" >BioEmu</a>
was featured on the front cover of [*Science*](https://www.science.org/doi/10.1126/science.adv9817).

I earned my 
<span class="small-caps">p</span>h.<span class="small-caps">d</span>.
in machine learning
from the
University of Cambridge, advised by 
Professor Richard E. Turner.
<!-- [Professor Richard E. Turner](https://rich-turner-group.github.io). -->
My research, combining probabilistic modeling and deep learning, was published at leading machine learning conferences including
<span class="small-caps" style="margin-right:-0.05em;">n</span>eur<span class="small-caps" style="margin-left: 0.03em;">ips</span>, <span class="small-caps">iclr</span>,
and
<span class="small-caps">icml</span>.

See my selected [papers](#publications), or visit my Google Scholar [profile](https://scholar.google.com/citations?user=2UOjgIUAAAAJ&hl=en) for a full list.

<span class="small-caps"> contact</span>:<span class="email">
foong.andrew@mayo.edu
</span>


<h1>Hiring News</h1>
I am hiring for a [research associate](https://jobs.mayoclinic.org/job/rochester/research-associate-multimodal-ai-for-oncology/33647/85322167136) in multimodal <span class="small-caps">ai</span> for oncology and a [research fellow](https://jobs.mayoclinic.org/job/rochester/research-fellow-ai-and-data-analytics-aida-radiation-oncology/33647/80077797392) in the Mayo Clinic <span class="small-caps">ai</span> and Data Analytics team. Candidates must have (or will soon be completing) a <span class="small-caps">p</span>h.<span class="small-caps">d</span>, and have <span class="small-caps">ai</span> expertise. Feel free to reach out via email if you have any questions.

<h1>Experience</h1>

<div style="display: flex; align-items: flex-start; margin-bottom: 1em;">
  <img src="../assets/mayo_clinic_logo.jpeg" alt="Mayo Clinic logo"
       style="width: 50px; height: 50px; margin-right: 15px; flex-shrink: 0; vertical-align: top; margin-top: 9px;" />
  <div style="margin-top: 0;">
    <p style="margin-top: 0;">
      <span class="small-caps">ai scientist & senior associate consultant<br> mayo clinic</span><br/>
      <span style="font-size: 1.0rem">
      January 2025–present <br> Rochester, Minnesota, <span class="small-caps">usa</span><br/>
      Department of Radiation Oncology<br/>
      <em><span class="small-caps">ai</span> for cancer treatment</em>
      </span>
    </p>
  </div>
</div>

<div style="display: flex; align-items: flex-start; margin-bottom: 1em;">
  <img src="../assets/msr.jpeg" alt="Microsoft logo"
       style="width: 50px; height: 50px; margin-right: 15px; flex-shrink: 0; vertical-align: top; margin-top: 9px;" />
  <div style="margin-top: 0;">
    <p style="margin-top: 0;">
      <span class="small-caps">senior researcher<br> microsoft research</span><br/>
      <span style="font-size: 1.0rem">
      November 2022–November 2024 <br> Cambridge, <span class="small-caps">uk</span><br/>
      <span class="small-caps">ai</span> for Science team<br/> 
      <em>Generative deep learning for protein structure</em>
      </span>
    </p>
  </div>
</div>

<div style="display: flex; align-items: flex-start; margin-bottom: 1em;">
  <img src="../assets/camlogo.png" alt="University of Cambridge logo"
       style="width: 50px; height: 50px; margin-right: 15px; flex-shrink: 0; vertical-align: top; margin-top: 9px;" />
  <div style="margin-top: 0;">
    <p style="margin-top: 0;">
      <span class="small-caps">ph.d. in machine learning<br> university of cambridge</span><br/>
      <span style="font-size: 1.0rem">
      October 2018–November 2022 <br> Cambridge, <span class="small-caps">uk</span><br/>
      Computational and Biological Learning Laboratory <br/>
      <em>Supervisor</em>: Professor Richard E. Turner<br/>
      <em>Deep learning</em> · <em>Probabilistic modeling</em>
     </span>
    </p>
  </div>
</div>

<div style="display: flex; align-items: flex-start; margin-bottom: 1em;">
  <img src="../assets/deepmind.png" alt="DeepMind logo"
       style="width: 50px; height: 50px; margin-right: 15px; flex-shrink: 0; vertical-align: top; margin-top: 9px;" />
  <div style="margin-top: 0;">
    <p style="margin-top: 0;">
      <span class="small-caps">research scientist intern<br> google deepmind</span><br/>
      <span style="font-size: 1.0rem">
      February 2022–May 2022 <br> London, <span class="small-caps">uk</span><br/>
      <em>Supervisor</em>: Dr. Michalis Titsias<br/>
      <em>Deep generative modeling</em> · <em>Gaussian processes</em>
      </span>
    </p>
  </div>
</div>

<div style="display: flex; align-items: flex-start; margin-bottom: 1em;">
  <img src="../assets/msr.jpeg" alt="Microsoft logo"
       style="width: 50px; height: 50px; margin-right: 15px; flex-shrink: 0; vertical-align: top; margin-top: 9px;" />
  <div style="margin-top: 0;">
    <p style="margin-top: 0;">
      <span class="small-caps">research intern<br> microsoft research</span><br/>
      <span style="font-size: 1.0rem">
      July 2021–October 2021 <br> Cambridge, <span class="small-caps">uk</span><br/>
      <em>Supervisor</em>: Dr. Sebastian Nowozin<br/>
      <em>Deep learning for molecular dynamics simulation</em>
      </span>
    </p>
  </div>
</div>

<div style="display: flex; align-items: flex-start; margin-bottom: 1em;">
  <img src="../assets/camlogo.png" alt="University of Cambridge logo"
       style="width: 50px; height: 50px; margin-right: 15px; flex-shrink: 0; vertical-align: top; margin-top: 9px;" />
  <div style="margin-top: 0;">
    <p style="margin-top: 0;">
      <span class="small-caps">b.a. & m.eng. in information and computer engineering<br> university of cambridge</span><br/>
      <span style="font-size: 1.0rem">
      October 2014–July 2018 <br> Cambridge, <span class="small-caps">uk</span><br/>
      Department of Engineering<br/>
      <em>First class honours with distinction, top 1–2% in year group</em>
      </span>
    </p>
  </div>
</div>

<!-- Custom styled HR -->
<hr class="hr-ghost">

<a id="publications" style="display: block; position: relative; top: -50px; visibility: hidden;"></a>
<h1>Research Papers</h1>

<div style="display:flex; align-items:flex-start; gap:1.2em; margin-bottom:0.5em;">
    <p style="margin:0; font-style: italic;">
      For a full list of research papers, see my
      <a href="https://scholar.google.com/citations?user=2UOjgIUAAAAJ&hl=en" target="_blank">
        Google Scholar page</a>. 
      Daggers<span style="font-style: normal;">†</span> denote co-first authorship.
    </p>
</div>


<div style="display:flex; align-items:flex-start; gap:1.2em; margin-bottom:0.5em;">
  <div style="margin-left: calc(50px + 0.8em);">
    <h2 style="margin:0;">2025</h2>
  </div>
</div>

<div style="display:flex; align-items:flex-start; gap:0.8em; margin-bottom:2em;">

  <!-- Square thumbnail -->
  <img src="/assets/images/publications/bioemu-banner.png"
       alt="BioEmu figure"
       style="width:50px; height:50px; object-fit:cover; border-radius:6px; flex-shrink:0; margin-top: 6px;">

  <!-- Text content -->
  <div>
    <!-- Remove default top margin so it lines up with the image -->
    <h3 style="margin:0 0 0.0em 0;">
      <a id="bioemu" style="display: block; position: relative; top: -50px; visibility: hidden;"></a>
      <a href="https://www.science.org/doi/10.1126/science.adv9817">
        Scalable Emulation of Protein Equilibrium Ensembles With Generative Deep Learning
      </a>
    </h3>

    <p style="margin:0 0 0.0em; font-style:italic;">
      Science
    </p>

    <p style="margin-bottom:0; margin-top:-0.8em; font-size:1.0rem">
      Sarah Lewis†, Tim Hempel†, José Jiménez-Luna†, Michael Gastegger†, Yu Xie†, <strong>Andrew Y. K. Foong</strong>†, Victor García Satorras†, Osama Abdin†, Bastiaan S. Veeling†, Iryna Zaporozhets, Yaoyi Chen, Soojung Yang, Arne Schneuing, Jigyasa Nigam, Federico Barbero, Vincent Stimper, Andrew Campbell, Jason Yim, Marten Lienen, Yu Shi, Shuxin Zheng, Hannes Schulz, Usman Munir, Cecilia Clementi, Frank Noé
    </p>

    <p>
    BioEmu is a generative deep-learning model that rapidly predicts the diverse shapes proteins adopt in nature, dramatically speeding up analyses that typically require lengthy molecular simulations. By efficiently uncovering functionally important protein movements, BioEmu accelerates drug discovery and provides a powerful computational tool to explore biological mechanisms previously inaccessible due to technical limitations.
    </p>

    <details style="margin-top:0.0em;">
      <summary style="cursor:pointer; margin-top:-0.8em; margin-top:-0.8em; font-variant: small-caps; font-size: 1.0rem;" class="small-caps">abstract</summary>
      <p style="margin-top:0.4em;">
       Following the sequence and structure revolutions, predicting the dynamical mechanisms of proteins that implement biological function remains an outstanding scientific challenge. Several experimental techniques and molecular dynamics <span class="small-caps">(md)</span> simulations can, in principle, determine conformational states, binding configurations and their probabilities, but suffer from low throughput. Here we develop a Biomolecular Emulator (BioEmu), a generative deep learning system that can generate thousands of statistically independent samples from the protein structure ensemble per hour on a single graphical processing unit. By leveraging novel training methods and vast data of protein structures, over 200 milliseconds of <span class="small-caps">md</span> simulation, and experimental protein stabilities, BioEmu’s protein ensembles represent equilibrium in a range of challenging and practically relevant metrics. Qualitatively, BioEmu samples many functionally relevant conformational changes, ranging from formation of cryptic pockets, over unfolding of specific protein regions, to large-scale domain rearrangements. Quantitatively, BioEmu samples protein conformations with relative free energy errors around 1 kcal/mol, as validated against millisecond-timescale <span class="small-caps">md</span> simulation and experimentally-measured protein stabilities. By simultaneously emulating structural ensembles and thermodynamic properties, BioEmu reveals mechanistic insights, such as the causes for fold destabilization of mutants, and can efficiently provide experimentally-testable hypotheses.
      </p>
    </details>
  </div>
</div>

<div style="display:flex; align-items:flex-start; gap:1.2em; margin-bottom:0.5em;">
  <div style="margin-left: calc(50px + 0.8em);">
    <h2 style="margin:0;">2023</h2>
  </div>
</div>

<div style="display:flex; align-items:flex-start; gap:0.8em; margin-bottom:2em;">

  <!-- Square thumbnail -->
  <img src="/assets/images/publications/frameflow-banner.png"
       alt="FrameFlow figure"
       style="width:50px; height:50px; object-fit:cover; border-radius:6px; flex-shrink:0; margin-top: 6px;">

  <!-- Text content -->
  <div>
    <h3 style="margin:0 0 0.3em 0;">
      <a href="https://www.mlsb.io/papers_2023/Fast_protein_backbone_generation_with_SE3_flow_matching.pdf">
        Fast Protein Backbone Generation with SE(3) Flow Matching
      </a>
    </h3>

    <p style="margin:0 0 0.3em; font-style:italic;">
      <span class="small-caps" style="margin-right:-0.05em;">n</span>eur<span class="small-caps" style="margin-left: 0.03em;">ips</span> Machine Learning for Structural Biology workshop <span class="small-caps">(mlsb)</span>
    </p>

    <p style="margin-bottom:0; margin-top:-0.8em; font-size: 1.0rem">
      Jason Yim, Andrew Campbell, <strong>Andrew Y. K. Foong</strong>, Michael Gastegger,
      José Jiménez-Luna, Sarah Lewis, Victor Garcia Satorras, Bastiaan S. Veeling,
      Regina Barzilay, Tommi Jaakkola, Frank Noé
    </p>

    <p>
    FrameFlow is a generative model that rapidly produces realistic protein backbones by using flow matching on the <span class="small-caps">se(</span>3<span class="small-caps">)</span> geometry group, significantly improving computational efficiency over previous diffusion-based approaches. By generating higher-quality proteins at a fraction of the computational cost, FrameFlow streamlines the design of novel proteins, making drug development and biological research more efficient.
    </p>

    <details style="margin-top:0.2em;">
      <summary style="cursor:pointer; margin-top:-0.8em; margin-top:-0.8em; font-variant: small-caps; font-size: 1.0rem;" class="small-caps">abstract</summary>
      <p style="margin-top:0.4em;">
        We present FrameFlow, a method for fast protein backbone generation using <span class="small-caps">se(</span>3<span class="small-caps">)</span> flow matching. Specifically, we adapt FrameDiff, a state-of-the-art diffusion model, to the flow-matching generative modeling paradigm. We show how flow matching can be applied on <span class="small-caps">se(</span>3<span class="small-caps">)</span> and propose modifications during training to effectively learn the vector field. Compared to FrameDiff, FrameFlow requires five times fewer sampling timesteps while achieving two fold better designability. The ability to generate high quality protein samples at a fraction of the cost of previous methods paves the way towards more efficient generative models in de novo protein design.
      </p>
    </details>
  </div>
</div>


<!-- Paper -->
<div style="display:flex; align-items:flex-start; gap:0.8em; margin-bottom:2em;">

  <!-- Square thumbnail -->
  <img src="/assets/images/publications/acnp-banner.png"
       alt="Autoregressive Conditional Neural Processes figure"
       style="width:50px; height:50px; object-fit:cover; border-radius:6px; flex-shrink:0; margin-top: 6px;">

  <!-- Text content -->
  <div>
    <h3 style="margin:0 0 0.3em 0;">
      <a href="https://openreview.net/forum?id=OAsXFPBfTBh">
        Autoregressive Conditional Neural Processes
      </a>
    </h3>

    <p style="margin:0 0 0.3em; font-style:italic;">
      International Conference on Learning Representations <span class="small-caps">(iclr)</span>
    </p>

    <p style="margin-bottom:0; margin-top:-0.8em; font-size: 1.0rem">
      Wessel Bruinsma†, Stratis Markou†, James Requeima†, <strong>Andrew Y. K. Foong</strong>†,
      Anna Vaughan, Tom Andersson, Anthony Buonomo, Scott Hosking, Richard E. Turner
    </p>

    <p>
    Autoregressive Conditional Neural Processes enhance the flexibility of Conditional Neural Processes (popular meta-learning models) by making predictions sequentially rather than independently, without complicating training or requiring approximate inference. This simple yet powerful approach enables accurate modeling of complex dependencies in data, achieving results competitive with sophisticated models at substantially reduced computational cost, which is particularly valuable in tasks like clinical time-series prediction.
    </p>

    <details style="margin-top:0.2em;">
      <summary style="cursor:pointer; margin-top:-0.8em; margin-top:-0.8em; font-variant: small-caps; font-size: 1.0rem;" class="small-caps">abstract</summary>
      <p style="margin-top:0.4em;">
        Conditional neural processes (<span class="small-caps">cnp</span>s; Garnelo et al., 2018a) are attractive meta-learning models which produce well-calibrated predictions and are trainable via a simple maximum likelihood procedure. Although <span class="small-caps">cnp</span>s have many advantages, they are unable to model dependencies in their predictions. Various works propose solutions to this, but these come at the cost of either requiring approximate inference or being limited to Gaussian predictions. In this work, we instead propose to change how <span class="small-caps">cnp</span>s are deployed at test time, without any modifications to the model or training procedure. Instead of making predictions independently for every target point, we autoregressively define a joint predictive distribution using the chain rule of probability, taking inspiration from the neural autoregressive density estimator <span class="small-caps">(nade)</span> literature. We show that this simple procedure allows factorised Gaussian <span class="small-caps">cnp</span>s to model highly dependent, non-Gaussian predictive distributions. Perhaps surprisingly, in an extensive range of tasks with synthetic and real data, we show that <span class="small-caps">cnp</span>s in autoregressive <span class="small-caps">(ar)</span> mode not only significantly outperform non-<span class="small-caps">ar</span> <span class="small-caps">cnp</span>s, but are also competitive with more sophisticated models that are significantly more computationally expensive and challenging to train.
      </p>
    </details>
  </div>
</div>


<!-- Paper -->
<div style="display:flex; align-items:flex-start; gap:0.8em; margin-bottom:2em;">

  <!-- Square thumbnail -->
  <img src="/assets/images/publications/timewarp-banner.png"
       alt="Timewarp figure"
       style="width:50px; height:50px; object-fit:cover; border-radius:6px; flex-shrink:0; margin-top: 6px;">

  <!-- Text content -->
  <div>
    <h3 style="margin:0 0 0.3em 0;">
      <a href="https://arxiv.org/abs/2302.01170">
        Timewarp: Transferable Acceleration of Molecular Dynamics by Learning Time-Coarsened Dynamics
      </a>
    </h3>

    <p style="margin:0 0 0.3em; font-style:italic;">
      Neural Information Processing Systems (<span class="small-caps" style="margin-right:-0.05em;">n</span>eur<span class="small-caps" style="margin-left: 0.03em;">ips</span>) spotlight presentation
    </p>

    <p style="margin-bottom:0; margin-top:-0.8em; font-size: 1.0rem">
      Leon Klein†, <strong>Andrew Y. K. Foong</strong>†, Tor Erlend Fjelde†, Bruno Mlodozeniec†,
      Marc Brockschmidt, Sebastian Nowozin, Frank Noé, Ryota Tomioka
    </p>

    <p>
    Timewarp accelerates molecular dynamics simulations by using machine learning to predict longer-time dynamics directly, enabling researchers to efficiently explore protein behavior occurring over biologically relevant timescales. Uniquely, its learned dynamics are transferable across different molecular systems, significantly reducing computational time and enabling rapid investigation of protein folding and binding processes relevant to drug discovery.
    </p>

    <details style="margin-top:0.2em;">
      <summary style="cursor:pointer; margin-top:-0.8em; margin-top:-0.8em; font-variant: small-caps; font-size: 1.0rem;" class="small-caps">abstract</summary>
      <p style="margin-top:0.4em;">
        Molecular dynamics <span class="small-caps">(md)</span> simulation is a widely used technique to simulate molecular systems, most commonly at the all-atom resolution where the equations of motion are integrated with timesteps on the order of femtoseconds (1&nbsp;fs = 10<sup>−15</sup>&nbsp;s). <span class="small-caps">md</span> is often used to compute equilibrium properties, which requires sampling from an equilibrium distribution such as the Boltzmann distribution. However, many important processes, such as binding and folding, occur over timescales of milliseconds or beyond, and cannot be efficiently sampled with conventional <span class="small-caps">md</span>. Furthermore, new <span class="small-caps">md</span> simulations need to be performed from scratch for each molecular system studied. We present Timewarp, an enhanced sampling method which uses a normalising flow as a proposal distribution in a Markov chain Monte Carlo method targeting the Boltzmann distribution. The flow is trained offline on <span class="small-caps">md</span> trajectories and learns to make large steps in time, simulating the molecular dynamics of 10<sup>5</sup>−10<sup>6</sup>&nbsp;fs. Crucially, Timewarp is transferable between molecular systems: once trained, we show that it generalises to unseen small peptides (2–4 amino acids), exploring their metastable states and providing wall-clock acceleration when sampling compared to standard <span class="small-caps">md</span>. Our method constitutes an important step towards developing general, transferable algorithms for accelerating <span class="small-caps">md</span>.
      </p>
    </details>
  </div>
</div>

<div style="display:flex; align-items:flex-start; gap:1.2em; margin-bottom:0.5em;">
  <div style="margin-left: calc(50px + 0.8em);">
    <h2 style="margin:0;">2022</h2>
  </div>
</div>


<!-- Paper -->
<div style="display:flex; align-items:flex-start; gap:0.8em; margin-bottom:2em;">

  <!-- Square thumbnail -->
  <img src="/assets/images/publications/thesis-banner.png"
       alt="Thesis figure"
       style="width:50px; height:50px; object-fit:cover; border-radius:6px; flex-shrink:0; margin-top: 6px;">

  <!-- Text content -->
  <div>
    <h3 style="margin:0 0 0.3em 0;">
      <a href="https://www.repository.cam.ac.uk/items/bb696578-0b93-44da-82e9-4896a1ab9266">
        Approximate Inference in Bayesian Neural Networks and Translation Equivariant Neural Processes
      </a>
    </h3>

    <p style="margin:0 0 0.3em; font-style:italic;">
      <span class="small-caps">p</span>h.<span class="small-caps">d</span>. thesis, University of Cambridge 
    </p>

    <p style="margin-bottom:0; margin-top:-0.8em; font-size: 1.0rem">
      <strong>Andrew Y. K. Foong</strong>
    </p>

    <p>
    This thesis investigates two probabilistic frameworks for modelling uncertainty in machine learning, Bayesian neural networks and neural processes, and highlights both theoretical limits and architectural advances. It shows where popular inference methods fall short in capturing uncertainty, and introduces Convolutional Neural Processes, which leverage spatial symmetries to improve predictions when structure in the data allows.
    </p>

    <details style="margin-top:0.2em;">
      <summary style="cursor:pointer; margin-top:-0.8em; margin-top:-0.8em; font-variant: small-caps; font-size: 1.0rem;" class="small-caps">abstract</summary>
      <p style="margin-top:0.4em;">
       It has been a longstanding goal in machine learning to develop flexible prediction methods that ‘know what they don’t know’—when faced with an out-of-distribution input, these models should signal their uncertainty rather than be confidently wrong. This thesis is concerned with two such probabilistic machine learning models: Bayesian neural networks and neural processes. Bayesian neural networks are a classical model that has been the subject of research since the 1990s. They rely on Bayesian inference to represent uncertainty in the weights of a neural network. On the other hand, neural processes are a recently introduced model that relies on meta-learning rather than Bayesian inference to obtain uncertainty estimates.

       This thesis provides contributions to both of these research areas. For Bayesian neural networks, we provide a theoretical and empirical study of the quality of common variational methods in approximating the Bayesian predictive distribution. We show that for single-hidden layer networks with <span class="small-caps">r</span>e<span class="small-caps">lu</span> activation functions, there are fundamental limitations concerning the representation of in-between uncertainty: increased uncertainty in between well separated regions of low uncertainty. We show that this theoretical limitation doesn’t apply for deeper networks. However, in practice, in-between uncertainty is a feature of the exact predictive distribution that is still often lost by approximate inference, even with deep networks.

       In the second part of this thesis, we focus on neural processes. In contrast to Bayesian neural networks, neural processes do not rely on approximate inference. Instead, they use neural networks to directly parameterise the map from a dataset to the posterior predictive stochastic process conditioned on that dataset. In this thesis we introduce the convolutional neural process, a new kind of neural process architecture which incorporates translation equivariance into its predictions. We show that when this symmetry is an appropriate assumption, convolutional neural processes outperform their standard multilayer perceptron-based and attentive counterparts on a variety of regression benchmarks. 
      </p>
    </details>
  </div>
</div>

<div style="display:flex; align-items:flex-start; gap:1.2em; margin-bottom:0.5em;">
  <div style="margin-left: calc(50px + 0.8em);">
    <h2 style="margin:0;">2021</h2>
  </div>
</div>


<!-- Paper -->
<div style="display:flex; align-items:flex-start; gap:0.8em; margin-bottom:2em;">

  <!-- Square thumbnail -->
  <img src="/assets/images/publications/bnn-collapsed-banner.png"
       alt="Collapsed Variational Bounds for bnns figure"
       style="width:50px; height:50px; object-fit:cover; border-radius:6px; flex-shrink:0; margin-top: 6px;">

  <!-- Text content -->
  <div>
    <h3 style="margin:0 0 0.3em 0;">
      <a href="https://openreview.net/forum?id=ykN3tbJ0qmX">
        Collapsed Variational Bounds for Bayesian Neural Networks
      </a>
    </h3>

    <p style="margin:0 0 0.3em; font-style:italic;">
      Neural Information Processing Systems (<span class="small-caps" style="margin-right:-0.05em;">n</span>eur<span class="small-caps" style="margin-left: 0.03em;">ips</span>)
    </p>

    <p style="margin-bottom:0; margin-top:-0.8em; font-size: 1.0rem">
      Marcin B. Tomczak, Siddharth Swaroop, <strong>Andrew Y. K. Foong</strong>, Richard E. Turner
    </p>

    <p>
    This paper introduces tighter variational bounds for Bayesian neural networks by treating prior parameters as latent variables and collapsing the bound analytically. The result is improved performance of mean-field variational inference in deep models along with a more principled way to learn hierarchical priors.
    </p>

    <details style="margin-top:0.2em;">
      <summary style="cursor:pointer; margin-top:-0.8em; margin-top:-0.8em; font-variant: small-caps; font-size: 1.0rem;" class="small-caps">abstract</summary>
      <p style="margin-top:0.4em;">
        Recent interest in learning large variational Bayesian Neural Networks <span class="small-caps">(bnn</span>s<span class="small-caps">)</span>  has been partly hampered by poor predictive performance caused by underfitting, and their performance is known to be very sensitive to the prior over weights. Current practice often fixes the prior parameters to standard values or tunes them using heuristics or cross-validation. In this paper, we treat prior parameters in a distributional way by extending the model and collapsing the variational bound with respect to their posteriors. This leads to novel and tighter Evidence Lower Bounds <span class="small-caps">(elbo</span>s<span class="small-caps">)</span> for performing variational inference <span class="small-caps">(vi)</span> in <span class="small-caps">bnn</span>s. Our experiments show that the new bounds significantly improve the performance of Gaussian mean-field <span class="small-caps">vi</span> applied to <span class="small-caps">bnn</span>s on a variety of data sets, demonstrating that mean-field <span class="small-caps">vi</span> works well even in deep models. We also find that the tighter <span class="small-caps">elbo</span>s can be good optimization targets for learning the hyperparameters of hierarchical priors.
      </p>
    </details>
  </div>
</div>

<!-- Paper -->
<div style="display:flex; align-items:flex-start; gap:0.8em; margin-bottom:2em;">

  <!-- Square thumbnail -->
  <img src="/assets/images/publications/pacbayes-smalldata-banner.png"
       alt="pac-Bayes Small Data figure"
       style="width:50px; height:50px; object-fit:cover; border-radius:6px; flex-shrink:0; margin-top: 6px;">

  <!-- Text content -->
  <div>
    <h3 style="margin:0 0 0.3em 0;">
      <a href="https://arxiv.org/abs/2106.03542">
        How Tight Can PAC-Bayes be in the Small Data Regime?
      </a>
    </h3>

    <p style="margin:0 0 0.3em; font-style:italic;">
      Neural Information Processing Systems (<span class="small-caps" style="margin-right:-0.05em;">n</span>eur<span class="small-caps" style="margin-left: 0.03em;">ips</span>)
    </p>    

    <p style="margin-bottom:0; margin-top:-0.8em; font-size: 1.0rem">
      <strong>Andrew Y. K. Foong</strong>†, Wessel P. Bruinsma†, David R. Burt, and Richard E. Turner
    </p>

    <p>
    This paper investigates how tight <span class="small-caps">pac</span>-Bayes bounds (tools used to estimate generalization error) can be when applied to small datasets, where each data point matters greatly. It reveals unexpected theoretical limits on <span class="small-caps">pac</span>-Bayes performance, demonstrating that while <span class="small-caps">pac</span>-Bayes bounds outperform standard test-set methods by using all available data, they still fall short of optimal test-set bounds in highly controlled scenarios.
    </p>

    <details style="margin-top:0.2em;">
      <summary style="cursor:pointer; margin-top:-0.8em; margin-top:-0.8em; font-variant: small-caps; font-size: 1.0rem;" class="small-caps">abstract</summary>
      <p style="margin-top:0.4em;">
        In this paper, we investigate the question: Given a small number of datapoints, for example <span class="small-caps">n</span> = 30, how tight can <span class="small-caps">pac</span>-Bayes and test set bounds be made? For such small datasets, test set bounds adversely affect generalisation performance by discarding data. In this setting, <span class="small-caps">pac</span>-Bayes bounds are especially attractive, due to their ability to use all the data to simultaneously learn a posterior and bound its generalisation risk. We focus on the case of i.i.d. data with a bounded loss and consider the generic <span class="small-caps">pac</span>-Bayes theorem of Germain et al. (2009) and Begin et al. (2016). While their theorem is known to recover many existing <span class="small-caps">pac</span>-Bayes bounds, it is unclear what the tightest bound derivable from their framework is. Surprisingly, we show that for a fixed learning algorithm and dataset, the tightest bound of this form coincides with the tightest bound of the more restrictive family of bounds considered in Catoni (2007). In contrast, in the more natural case of distributions over datasets, we give examples (both analytic and numerical) showing that the family of bounds in Catoni (2007) can be suboptimal. Within the proof framework of Germain et al. (2009) and Begin et al. (2016), we establish a lower bound on the best bound achievable in expectation, which recovers the Chernoff test set bound in the case when the posterior is equal to the prior. Finally, to illustrate how tight these bounds can potentially be, we study a synthetic one-dimensional classification task in which it is feasible to meta-learn both the prior and the form of the bound to obtain the tightest <span class="small-caps">pac</span>-Bayes and test set bounds possible. We find that in this simple, controlled scenario, <span class="small-caps">pac</span>-Bayes bounds are surprisingly competitive with comparable, commonly used Chernoff test set bounds. However, the sharpest test set bounds still lead to better guarantees on the generalisation error than the <span class="small-caps">pac</span>-Bayes bounds we consider.
      </p>
    </details>
  </div>
</div>


<!-- Paper -->
<div style="display:flex; align-items:flex-start; gap:0.8em; margin-bottom:2em;">

  <!-- Square thumbnail -->
  <img src="/assets/images/publications/gnp-banner.png"
       alt="Gaussian Neural Process figure"
       style="width:50px; height:50px; object-fit:cover; border-radius:6px; flex-shrink:0; margin-top: 6px;">

  <!-- Text content -->
  <div>
    <h3 style="margin:0 0 0.3em 0;">
      <a href="https://arxiv.org/abs/2101.03606">
        The Gaussian Neural Process
      </a>
    </h3>

    <p style="margin:0 0 0.3em; font-style:italic;">
      Advances in Approximate Bayesian Inference <span class="small-caps">(aabi)</span>
    </p>    

    <p style="margin-bottom:0; margin-top:-0.8em; font-size: 1.0rem">
      Wessel P. Bruinsma, James Requeima, <strong>Andrew Y. K. Foong</strong>,
      Jonathan Gordon, and Richard E. Turner
    </p>

    <p>
    This paper introduces the Gaussian Neural Process, a new model that captures correlations in predictions and incorporates translation symmetry. By addressing key limitations of existing Neural Processes, the Gaussian Neural Process improves predictive accuracy, broadening their applicability in meta-learning and tasks requiring robust uncertainty estimation.
    </p>

    <details style="margin-top:0.2em;">
      <summary style="cursor:pointer; margin-top:-0.8em; margin-top:-0.8em; font-variant: small-caps; font-size: 1.0rem;" class="small-caps">abstract</summary>
      <p style="margin-top:0.4em;">
        Neural Processes (<span class="small-caps">np</span>s; Garnelo et al., 2018a,b) are a rich class of models for meta-learning that map data sets directly to predictive stochastic processes. We provide a rigorous analysis of the standard maximum-likelihood objective used to train conditional <span class="small-caps">np</span>s. Moreover, we propose a new member to the Neural Process family called the Gaussian Neural Process <span class="small-caps">(gnp)</span>, which models predictive correlations, incorporates translation equivariance, provides universal approximation guarantees, and demonstrates encouraging performance.
      </p>
    </details>
  </div>
</div>



<div style="display:flex; align-items:flex-start; gap:1.2em; margin-bottom:0.5em;">
  <div style="margin-left: calc(50px + 0.8em);">
    <h2 style="margin:0;">2020</h2>
  </div>
</div>


<div style="display:flex; align-items:flex-start; gap:0.8em; margin-bottom:2em;">

  <!-- Square thumbnail -->
  <img src="/assets/images/publications/convnp-banner.png"
       alt="ConvNP figure"
       style="width:50px; height:50px; object-fit:cover; border-radius:6px; flex-shrink:0; margin-top: 6px;">

  <!-- Text content -->
  <div>
    <h3 style="margin:0 0 0.3em 0;">
      <a href="https://arxiv.org/abs/2007.01332">
        Meta-Learning Stationary Stochastic Process Prediction with Convolutional Neural Processes
      </a>
    </h3>

    <p style="margin:0 0 0.3em; font-style:italic;">
      Neural Information Processing Systems (<span class="small-caps" style="margin-right:-0.05em;">n</span>eur<span class="small-caps" style="margin-left: 0.03em;">ips</span>)
    </p>

    <p style="margin-bottom:0; margin-top:-0.8em; font-size: 1.0rem">
      <strong>Andrew Y. K. Foong</strong>†, Wessel P. Bruinsma†, Jonathan Gordon†, Yann Dubois,
      James Requeima, and Richard E. Turner
    </p>

    <p>
    This follow-up paper builds on Conv<span class="small-caps">cnp</span>s by enabling predictions that capture dependencies in structured data, improving coherence in tasks like image completion and spatio-temporal forecasting. Conv<span class="small-caps">np</span>s also introduce a simpler and more effective training approach for latent variable models compared with previous latent neural process models.
    </p>

    <details style="margin-top:0.2em;">
      <summary style="cursor:pointer; margin-top:-0.8em; margin-top:-0.8em; font-variant: small-caps; font-size: 1.0rem;" class="small-caps">abstract</summary>
      <p style="margin-top:0.4em;">
        Stationary stochastic processes <span class="small-caps">(sp</span>s<span class="small-caps">)</span> are a key component of many probabilistic models, such as those for off-the-grid spatio-temporal data. They enable the statistical symmetry of underlying physical phenomena to be leveraged, thereby aiding generalization. Prediction in such models can be viewed as a translation equivariant map from observed data sets to predictive <span class="small-caps">sp</span>s, emphasizing the intimate relationship between stationarity and equivariance. Building on this, we propose the Convolutional Neural Process (Conv<span class="small-caps">np</span>), which endows Neural Processes <span class="small-caps">(np</span>s<span class="small-caps">)</span> with translation equivariance and extends convolutional conditional <span class="small-caps">np</span>s to allow for dependencies in the predictive distribution. The latter enables Conv<span class="small-caps">np</span>s to be deployed in settings which require coherent samples, such as Thompson sampling or conditional image completion. Moreover, we propose a new maximum-likelihood objective to replace the standard <span class="small-caps">elbo</span> objective in <span class="small-caps">np</span>s, which conceptually simplifies the framework and empirically improves performance. We demonstrate the strong performance and generalization capabilities of Conv<span class="small-caps">np</span>s on 1<span class="small-caps">d</span> regression, image completion, and various tasks with real-world spatio-temporal data.
      </p>
    </details>
  </div>
</div>


<!-- Paper -->
<div style="display:flex; align-items:flex-start; gap:0.8em; margin-bottom:2em;">

  <!-- Square thumbnail -->
  <img src="/assets/images/publications/convcnp-banner.png"
       alt="Convolutional Conditional Neural Processes figure"
       style="width:50px; height:50px; object-fit:cover; border-radius:6px; flex-shrink:0; margin-top: 6px;">

  <!-- Text content -->
  <div>
    <h3 style="margin:0 0 0.3em 0;">
      <a href="https://arxiv.org/abs/1910.13556">
        Convolutional Conditional Neural Processes
      </a>
    </h3>

    <p style="margin:0 0 0.3em; font-style:italic;">
      International Conference on Learning Representations (<span class="small-caps">iclr</span>)
    </p>

    <p style="margin-bottom:0; margin-top:-0.8em; font-size: 1.0rem">
      Jonathan Gordon†, Wessel P. Bruinsma†, <strong>Andrew Y. K. Foong</strong>,
      James Requeima, Yann Dubois, and Richard E. Turner
    </p>

    <p>
    This paper introduces the Convolutional Conditional Neural Process (Conv<span class="small-caps">cnp</span>), a neural-network alternative to Gaussian processes, designed for reliable predictions when data is scarce or uncertainty matters. By incorporating translation symmetry, Conv<span class="small-caps">cnp</span>s accurately capture patterns in structured data, such as images or time series, and provides meaningful uncertainty estimates even on tasks it hasn't encountered before.
    </p>

    <details style="margin-top:0.2em;">
      <summary style="cursor:pointer; margin-top:-0.8em; margin-top:-0.8em; font-variant: small-caps; font-size: 1.0rem;" class="small-caps">abstract</summary>
      <p style="margin-top:0.4em;">
        We introduce the Convolutional Conditional Neural Process (Conv<span class="small-caps">cnp</span>), a new member of the Neural Process family that models translation equivariance in the data. Translation equivariance is an important inductive bias for many learning problems including time series modeling, spatial data, and images. The model embeds data sets into an infinite-dimensional function space as opposed to a finite-dimensional vector space. To formalize this notion, we extend the theory of neural representations of sets to include functional representations, and demonstrate that any translation-equivariant embedding can be represented using a convolutional deep set. We evaluate Conv<span class="small-caps">cnp</span>s in several settings, demonstrating that they achieve state-of-the-art performance compared to existing <span class="small-caps">np</span>s. We demonstrate that building in translation equivariance enables zero-shot generalization to challenging, out-of-domain tasks.
      </p>
    </details>
  </div>
</div>


<!-- Paper -->
<div style="display:flex; align-items:flex-start; gap:0.8em; margin-bottom:2em;">

  <!-- Square thumbnail -->
  <img src="/assets/images/publications/expressiveness-banner.png"
       alt="bnn figure"
       style="width:50px; height:50px; object-fit:cover; border-radius:6px; flex-shrink:0; margin-top: 6px;">

  <!-- Text content -->
  <div>
    <h3 style="margin:0 0 0.3em 0;">
      <a href="https://arxiv.org/abs/1909.00719">
        On the Expressiveness of Approximate Inference in Bayesian Neural Networks
      </a>
    </h3>

    <p style="margin:0 0 0.3em; font-style:italic;">
      Neural Information Processing Systems (<span class="small-caps" style="margin-right:-0.05em;">n</span>eur<span class="small-caps" style="margin-left: 0.03em;">ips</span>)
    </p>

    <p style="margin-bottom:0; margin-top:-0.8em; font-size:1.0rem">
      <strong>Andrew Y. K. Foong</strong>†, David R. Burt†, Yingzhen Li, Richard E. Turner
    </p>

    <p>
    This paper explores critical limitations of commonly-used approximation methods in Bayesian neural networks <span class="small-caps">(bnn</span>s<span class="small-caps">)</span>, highlighting cases where these methods fail to represent uncertainty accurately. It reveals that although deeper networks theoretically overcome some limitations, problems persist in practice, emphasizing the need for caution when using approximate inference to obtain reliable uncertainty estimates from <span class="small-caps">bnn</span>s.
    </p>

    <details style="margin-top:0.2em;">
      <summary style="cursor:pointer; margin-top:-0.8em; margin-top:-0.8em; font-variant: small-caps; font-size: 1.0rem;" class="small-caps">abstract</summary>
      <p style="margin-top:0.4em;">
        While Bayesian neural networks <span class="small-caps">(bnn</span>s<span class="small-caps">)</span> hold the promise of being flexible, well-calibrated statistical models, inference often requires approximations whose consequences are poorly understood. We study the quality of common variational methods in approximating the Bayesian predictive distribution. For single-hidden layer <span class="small-caps">r</span>e<span class="small-caps">lu</span> <span class="small-caps">bnn</span>s, we prove a fundamental limitation in function-space of two of the most commonly used distributions defined in weight-space: mean-field Gaussian and Monte Carlo dropout. We find there are simple cases where neither method can have substantially increased uncertainty in between well-separated regions of low uncertainty. We provide strong empirical evidence that exact inference does not have this pathology, hence it is due to the approximation and not the model. In contrast, for deep networks, we prove a universality result showing that there exist approximate posteriors in the above classes which provide flexible uncertainty estimates. However, we find empirically that pathologies of a similar form as in the single-hidden layer case can persist when performing variational inference in deeper networks. Our results motivate careful consideration of the implications of approximate inference methods in <span class="small-caps">bnn</span>s.
      </p>
    </details>
  </div>
</div>


<div style="display:flex; align-items:flex-start; gap:1.2em; margin-bottom:0.5em;">
  <div style="margin-left: calc(50px + 0.8em);">
    <h2 style="margin:0;">2019</h2>
  </div>
</div>


<!-- Paper -->
<div style="display:flex; align-items:flex-start; gap:0.8em; margin-bottom:2em;">

  <!-- Square thumbnail -->
  <img src="/assets/images/publications/inbetween-uncertainty-banner.png"
       alt="In-Between Uncertainty in Bayesian Neural Networks figure"
       style="width:50px; height:50px; object-fit:cover; border-radius:6px; flex-shrink:0; margin-top: 6px;">

  <!-- Text content -->
  <div>
    <h3 style="margin:0 0 0.3em 0;">
      <a href="https://arxiv.org/abs/1906.11537">
        <span style="display: inline-block; margin-left: -0.47em;">“</span>In-Between” Uncertainty in Bayesian Neural Networks
      </a>
    </h3>

    <p style="margin:0 0 0.3em; font-style:italic;">
      <span class="small-caps">icml</span> Workshop on Uncertainty and Robustness in Deep Learning
    </p>

    <p style="margin-bottom:0; margin-top:-0.8em; font-size: 1.0rem">
      <strong>Andrew Y. K. Foong</strong>, Yingzhen Li, José Miguel Hernández-Lobato, and Richard E. Turner
    </p>

    <p>
    This paper identifies a critical flaw in how popular Bayesian neural network methods (like mean-field variational inference) estimate uncertainty, particularly when predicting outside known data regions. It shows that classical approaches, like the linearised Laplace approximation, handle such “in-between” uncertainty more reliably, improving trustworthiness in safety-critical applications.
    </p>

    <details style="margin-top:0.2em;">
      <summary style="cursor:pointer; margin-top:-0.8em; margin-top:-0.8em; font-variant: small-caps; font-size: 1.0rem;" class="small-caps">abstract</summary>
      <p style="margin-top:0.4em;">
        We describe a limitation in the expressiveness of the predictive uncertainty estimate given by mean-field variational inference <span class="small-caps">(mfvi)</span>, a popular approximate inference method for Bayesian neural networks. In particular, <span class="small-caps">mfvi</span> fails to give calibrated uncertainty estimates in between separated regions of observations. This can lead to catastrophically overconfident predictions when testing on out-of-distribution data. Avoiding such overconfidence is critical for active learning, Bayesian optimisation and out-of-distribution robustness. We instead find that a classical technique, the linearised Laplace approximation, can handle “in-between” uncertainty much better for small network architectures.
      </p>
    </details>
  </div>
</div>


<hr class="hr-ghost">

<a id="talks" style="display: block; position: relative; top: -50px; visibility: hidden;"></a>
<h1 id="talks">Talks</h1>

<div style="display:flex; align-items:flex-start; gap:1.2em; margin-bottom:0.5em;">
    <p style="margin:0; font-style: italic;">
      A collection of my publicly available presentations and slide decks.
    </p>
</div>


<div style="display:flex; align-items:flex-start; gap:1.2em; margin-bottom:1em;">
  <div style="margin-left: calc(50px + 0.8em);">
    <h2 style="margin:0;">2025</h2>
  </div>
</div>


<a id="ai-from-scratch" style="display: block; position: relative; top: -50px; visibility: hidden;"></a>

<!-- Header row: icon + title + description -->
<div style="margin-bottom: 2em;">
  <div style="display:flex; align-items:flex-start; gap:0.8em;">
    <!-- Small icon placeholder -->
    <img src="/assets/images/talks/ai-from-scratch.png"
     alt="Icon"
     style="width: 50px; height: 50px; object-fit: cover; border-radius: 6px; flex-shrink: 0; margin-top: 6px;">

    <div style="flex: 1;">
      <h3 style="margin-top: 0; margin-bottom: 0.1em;">
        Understanding AI from Scratch
      </h3>

      <p style="margin: 0; font-size: 1.0rem; font-variant: small-caps;" class="small-caps">
        mayo clinic lecture series
      </p>

      <p style="margin-top: 0.6em;">
        I designed this six-part series to help clinicians at the Mayo Clinic with no prior <span class="small-caps">ai</span> experience understand deep learning from first principles. 
      </p>

      <p class="hanging-indent" style="margin-top: 0.6em;">
       <span class="small-caps">lectures 1 &amp; 2</span> cover deep learning from the ground up, from linear regression and gradient descent to neural networks, overfitting, and generalization.
      </p>

      <p style="margin-top: 0.6em;" class="hanging-indent">
      <span class="small-caps">lectures 3 &amp; 4</span> explain what convolutional neural networks are and how they can be used to understand images.
      </p>

      <p style="margin-top: 0.6em;" class="hanging-indent">
      <span class="small-caps">lectures 5 &amp; 6</span> explain how Chat<span class="small-caps">gpt</span> works, how it was made, and how prompting and retrieval-augmented generation <span class="small-caps">(rag)</span> can increase accuracy.
      </p>

    </div>
  </div>

  <!-- Thumbnail Grid: aligned with text block, centered within it -->
  <div style="
    margin-top: 1.5em;
    margin-left: calc(50px + 0.8em);
    max-width: calc(100% - 50px - 0.8em);
    display: flex;
    flex-wrap: wrap;
    gap: 30px;
    justify-content: center;
  ">

    <!-- Lecture blocks -->
    <div style="text-align: center; width: 200px;">
      <a href="https://mssvideoupload.mayo.edu/playlist/dedicated/1_uq8gpkab/1_rhvbtb46" target="_blank">
        <img src="/assets/lectures/lecture1-thumb.jpg" alt="Lecture 1 thumbnail" style="width: 100%; border-radius: 8px;">
      </a>
      <div style="margin-top: 0.5em; font-size: 1.0em;">
        <span class="small-caps">lecture 1</span> <br> <span style="font-style: italic">What is Deep Learning?</span>
      </div>
    </div>

    <div style="text-align: center; width: 200px;">
      <a href="https://mssvideoupload.mayo.edu/playlist/dedicated/1_uq8gpkab/1_xbkcrbjs" target="_blank">
        <img src="/assets/lectures/lecture2-thumb.jpg" alt="Lecture 2 thumbnail" style="width: 100%; border-radius: 8px;">
      </a>
      <div style="margin-top: 0.5em; font-size: 1.0em;">
        <span class="small-caps">lecture 2</span> <br> <span style="font-style: italic">From Single Neurons to Neural Networks</span>
      </div>
    </div>

    <div style="text-align: center; width: 200px;">
      <a href="https://mssvideoupload.mayo.edu/playlist/dedicated/1_uq8gpkab/1_2q24t2cy" target="_blank">
        <img src="/assets/lectures/lecture3-thumb.jpg" alt="Lecture 3 thumbnail" style="width: 100%; border-radius: 8px;">
      </a>
      <div style="margin-top: 0.5em; font-size: 1.0em;">
        <span class="small-caps">lecture 3</span> <br><span style="font-style: italic"><span class="small-caps">ai</span> for Imaging</span>
      </div>
    </div>

    <div style="text-align: center; width: 200px;">
      <a href="https://mssvideoupload.mayo.edu/playlist/dedicated/1_uq8gpkab/1_4m5awnik" target="_blank">
        <img src="/assets/lectures/lecture4-thumb.jpg" alt="Lecture 4 thumbnail" style="width: 100%; border-radius: 8px;">
      </a>
      <div style="margin-top: 0.5em; font-size: 1.0em;">
        <span class="small-caps">lecture 4</span> <br><span style="font-style: italic">Practical <span class="small-caps">ai</span> for Imaging</span>
      </div>
    </div>

    <div style="text-align: center; width: 200px;">
      <a href="https://mssvideoupload.mayo.edu/playlist/dedicated/1_uq8gpkab/1_h9wwrlo9" target="_blank">
        <img src="/assets/lectures/lecture5-thumb.jpg" alt="Lecture 5 thumbnail" style="width: 100%; border-radius: 8px;">
      </a>
      <div style="margin-top: 0.5em; font-size: 1.0em;">
        <span class="small-caps">lecture 5</span> <br><span style="font-style: italic">How Does Chat<span class="small-caps">gpt</span> Work?</span>
      </div>
    </div>

    <div style="text-align: center; width: 200px;">
      <a href="https://mssvideoupload.mayo.edu/playlist/dedicated/1_uq8gpkab/1_euus6hqk" target="_blank">
        <img src="/assets/lectures/lecture6-thumb.jpg" alt="Lecture 6 thumbnail" style="width: 100%; border-radius: 8px;">
      </a>
      <div style="margin-top: 0.5em; font-size: 1.0em;">
        <span class="small-caps">lecture 6</span> <br><span style="font-style: italic">Prompting Chat<span class="small-caps">gpt</span></span>
      </div>
    </div>

  </div>
</div>





<div style="display:flex; align-items:flex-start; gap:1.2em; margin-bottom:1em;">
  <div style="margin-left: calc(50px + 0.8em);">
    <h2 style="margin:0;">2023</h2>
  </div>
</div>



<div style="margin-bottom: 2em;">
  <!-- Header row: icon + title + description -->
  <div style="display:flex; align-items:flex-start; gap:0.8em;">
    <img src="/assets/images/publications/timewarp-banner.png"
     alt="Icon"
     style="width: 50px; height: 50px; object-fit: cover; border-radius: 6px; flex-shrink: 0; margin-top: 6px;">

    <div style="flex: 1;">
      <h3 style="margin-top: 0em; margin-bottom: 0.1em;">
        Timewarp: Transferable Acceleration of Molecular Dynamics by Learning Time-Coarsened Dynamics
      </h3>

      <span class="small-caps">
        online reading group presentation
      </span>

      <p style="margin-top: 0.6em;">
        Co-first author <a href="https://www.mi.fu-berlin.de/en/math/groups/ai4s/staff/klein/index.html" target="_blank" rel="noopener noreferrer">Leon Klein</a> and I presented our <span class="small-caps" style="margin-right:-0.05em;">n</span>eur<span class="small-caps" style="margin-left: 0.03em;">ips</span> 2023 spotlight paper on using deep learning to accelerate molecular dynamics simulation at two online reading groups.
      </p>
    </div>
  </div>

  <!-- Video 1 -->
  <div style="margin-top: 1em; margin-left: calc(50px + 0.8em); max-width: calc(100% - 50px - 0.8em);">
    <div style="aspect-ratio: 16 / 9; width: 100%;">
      <iframe 
        src="https://www.youtube.com/embed/4rtT-hE9Xqo?si=tfpzjzFw9n-gbMp_"
        title="YouTube video player"
        style="width: 100%; height: 100%; border: none;"
        allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
        referrerpolicy="strict-origin-when-cross-origin"
        allowfullscreen>
      </iframe>
    </div>
    <div style="font-style: italic; margin-top: 0.4em; text-align: center;">
      Learning on Graphs and Geometry <span class="small-caps">(l</span>o<span class="small-caps">gg)</span>
    </div>
  </div>

  <!-- Video 2 -->
  <div style="margin-top: 2em; margin-left: calc(50px + 0.8em); max-width: calc(100% - 50px - 0.8em);">
    <div style="aspect-ratio: 16 / 9; width: 100%;">
      <iframe 
        src="https://www.youtube.com/embed/fD_1V5HgGTQ"
        title="YouTube video player"
        style="width: 100%; height: 100%; border: none;"
        allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
        referrerpolicy="strict-origin-when-cross-origin"
        allowfullscreen>
      </iframe>
    </div>
    <div style="font-style: italic; margin-top: 0.4em; text-align: center;">
      Molecular Modeling and Drug Discovery <span class="small-caps">(m</span>2<span class="small-caps">d</span>2<span class="small-caps">)</span>
    </div>
  </div>
</div>



<div style="display:flex; align-items:flex-start; gap:1.2em; margin-bottom:1em;">
  <div style="margin-left: calc(50px + 0.8em);">
    <h2 style="margin:0;">2021</h2>
  </div>
</div>



<div style="margin-bottom: 2em;">
  <!-- Header row: icon + title + description -->
  <div style="display:flex; align-items:flex-start; gap:0.8em;">
    <img src="/assets/images/talks/pac-bayes.png"
     alt="Icon"
     style="width: 50px; height: 50px; object-fit: cover; border-radius: 6px; flex-shrink: 0; margin-top: 6px;">

    <div style="flex: 1;">
      <h3 style="margin-top: 0em; margin-bottom: 0.1em;">
        An Introduction to PAC-Bayes
      </h3>

      <span class="small-caps">
        cambridge machine learning reading group
      </span>

      <p style="margin-top: 0.6em;">
        I gave an introductory talk on the statistical learning framework and <span class="small-caps">pac</span>-Bayes with <a href="https://davidrburt.github.io/" target="_blank" rel="noopener noreferrer">David Burt</a> and <a href="https://javierantoran.github.io/about/" target="_blank" rel="noopener noreferrer">Javier Antoran</a>. View the <a href="../files/pac_bayes_reading_group.pdf" target="_blank">slides</a>.
      </p>
    </div>
  </div>

  <!-- Aligned YouTube embed -->
  <div style="margin-top: 1em; margin-left: calc(50px + 0.8em); max-width: calc(100% - 50px - 0.8em);">
    <div style="aspect-ratio: 16 / 9; width: 100%;">
      <iframe 
        src="https://www.youtube.com/embed/t5GBuBD0ibc"
        title="YouTube video player"
        style="width: 100%; height: 100%; border: none;"
        allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
        referrerpolicy="strict-origin-when-cross-origin"
        allowfullscreen>
      </iframe>
    </div>
  </div>
</div>



<div style="margin-bottom: 2em;"></div>



<div style="margin-bottom: 2em;">
  <!-- Header row: icon + title + description -->
  <div style="display:flex; align-items:flex-start; gap:0.8em;">
    <img src="/assets/images/talks/oxford.png"
     alt="Icon"
     style="width: 50px; height: 50px; object-fit: cover; border-radius: 6px; flex-shrink: 0; margin-top: 6px;">

    <div style="flex: 1;">
      <h3 style="margin-top: 0em; margin-bottom: 0.1em;">
        Understanding Approximate Inference in Bayesian Neural Networks
      </h3>

      <span class="small-caps">
        joint talk with oxford university
      </span>

      <p style="margin-top: 0.6em;">
        I gave a talk on my <a href="https://arxiv.org/abs/1909.00719" target="_blank" rel="noopener noreferrer"><span class="small-caps" style="margin-right:-0.05em;">n</span>eur<span class="small-caps" style="margin-left: 0.03em;">ips</span> 2020 paper</a> on approximate inference in Bayesian neural networks, with an accompanying talk by <a href="https://sebastianfarquhar.com/" target="_blank" rel="noopener noreferrer">Sebastian Farquhar</a> of the University of Oxford. Our talks present different perspectives on the effectiveness of the mean-field approximation in these models. View the <a href="../files/BNNs_talk.pdf" target="_blank">slides</a>.
      </p>
    </div>
  </div>

  <!-- Aligned YouTube embed -->
  <div style="margin-top: 1em; margin-left: calc(50px + 0.8em); max-width: calc(100% - 50px - 0.8em);">
    <div style="aspect-ratio: 16 / 9; width: 100%;">
      <iframe 
        src="https://www.youtube.com/embed/BJTkLxSQrHI"
        title="YouTube video player"
        style="width: 100%; height: 100%; border: none;"
        allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
        referrerpolicy="strict-origin-when-cross-origin"
        allowfullscreen>
      </iframe>
    </div>
  </div>
</div>



<div style="display:flex; align-items:flex-start; gap:1.2em; margin-bottom:1em;">
  <div style="margin-left: calc(50px + 0.8em);">
    <h2 style="margin:0;">2020</h2>
  </div>
</div>


<div style="margin-bottom: 4em;">
  <!-- Header row: icon + title + description -->
  <div style="display:flex; align-items:flex-start; gap:0.8em;">
    <img src="/assets/images/publications/expressiveness-banner.png"
     alt="Icon"
     style="width: 50px; height: 50px; object-fit: cover; border-radius: 6px; flex-shrink: 0; margin-top: 6px;">

    <div style="flex: 1;">
      <h3 style="margin-top: 0em; margin-bottom: 0.1em;">
        On the Expressiveness of Approximate Inference in Bayesian Neural Networks
      </h3>

      <span class="small-caps">
        presentation at neurips conference
      </span>

      <p style="margin-top: 0.6em;">
        A short video describing my paper on flaws in uncertainty estimation when using common approximate Bayesian neural network inference methods.
      </p>
    </div>
  </div>

  <!-- Responsive video embed -->
  <div class="embed-wrapper" style="line-height: 0; margin-top: 1em; margin-left: calc(50px + 0.8em); position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden; max-width: calc(100% - 50px - 0.8em);">
    <div id="presentation-embed-38937338"
        style="position: absolute; top: 0; left: 0; width: 100%; height: 100%;">
    </div>
  </div>

  <script src="https://slideslive.com/embed_presentation.js"></script>
  <script>
    embed = new SlidesLiveEmbed("presentation-embed-38937338", {
      presentationId: "38937338",
      autoPlay: false,
      verticalEnabled: false,
    });
  </script>
</div>




<!-- Talk container -->
<div style="margin-bottom: 4em;">
  <!-- Header row: icon + title + description -->
  <div style="display:flex; align-items:flex-start; gap:0.8em;">
    <img src="/assets/images/publications/convnp-banner.png"
     alt="Icon"
     style="width: 50px; height: 50px; object-fit: cover; border-radius: 6px; flex-shrink: 0; margin-top: 6px;">

    <div style="flex: 1;">
      <h3 style="margin-top: 0em; margin-bottom: 0.1em;">
        Meta-learning Stationary Stochastic Process Prediction with Convolutional Neural Processes
      </h3>

      <span class="small-caps">
        presentation at neurips conference
      </span>

      <p style="margin-top: 0.6em;">
        A short video explaining my paper on neural processes, a deep learning alternative to Gaussian processes for regression problems with uncertainty.
      </p>
    </div>
  </div>

  <!-- Responsive video embed -->
  <div class="embed-wrapper" style="line-height: 0; margin-top: 1em; margin-left: calc(50px + 0.8em); position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden; max-width: calc(100% - 50px - 0.8em);">
    <div id="presentation-embed-38937329"
        style="position: absolute; top: 0; left: 0; width: 100%; height: 100%;">
    </div>
  </div>

  <script src="https://slideslive.com/embed_presentation.js"></script>
  <script>
    embed = new SlidesLiveEmbed("presentation-embed-38937329", {
      presentationId: "38937329",
      autoPlay: false,
      verticalEnabled: false,
    });
  </script>
</div>




<!-- Talk: Neural Processes -->
<div style="display:flex; align-items:flex-start; gap:0.8em; margin-bottom:2em;">
  <!-- Icon placeholder -->
    <img src="/assets/images/blogs/npf-icon.png"
     alt="Icon"
     style="width: 50px; height: 50px; object-fit: cover; border-radius: 6px; flex-shrink: 0; margin-top: 6px;">

  <div>
    <h3 style="margin-top: 0em; margin-bottom: 0.1em;">
      Neural Processes
    </h3>

    <span class="small-caps">
      cambridge machine learning reading group
    </span>

    <p style="margin-top: 0.6em;">
      A reading group talk given with <a href="https://twitter.com/sebastian_ober?lang=en" target="_blank" rel="noopener noreferrer">Sebastian Ober</a> and Stratis Markou, introducing various neural processes and covering much of the material in <a href="https://yanndubs.github.io/Neural-Process-Family/text/Intro.html" target="_blank" rel="noopener noreferrer">this blog post</a>. View the <a href="../files/nps_reading_group.pdf" target="_blank">slides</a>.
    </p>
  </div>
</div>

<!-- Talk: Recent Advances in Bayesian Deep Learning -->
<div style="display:flex; align-items:flex-start; gap:0.8em; margin-bottom:2em;">
  <!-- Icon placeholder -->
    <img src="/assets/images/talks/bdl.png"
     alt="Icon"
     style="width: 50px; height: 50px; object-fit: cover; border-radius: 6px; flex-shrink: 0; margin-top: 6px;">

  <div>
    <h3 style="margin-top: 0em; margin-bottom: 0.1em;">
      Recent Advances in Bayesian Deep Learning
    </h3>

    <span class="small-caps">
      cambridge machine learning reading group
    </span>

    <p style="margin-top: 0.6em;">
      A reading group talk given with <a href="https://siddharthswaroop.github.io/" target="_blank" rel="noopener noreferrer">Siddharth Swaroop</a>, covering modern stochastic gradient Markov chain Monte Carlo <span class="small-caps">(sgmcmc)</span> and natural gradient variational inference methods for Bayesian deep learning. View the <a href="../files/Recent_Advances_in_Bayesian_Deep_Learning.pdf" target="_blank">slides</a>.
    </p>
  </div>
</div>


<div style="display:flex; align-items:flex-start; gap:1.2em; margin-bottom:1em;">
  <div style="margin-left: calc(50px + 0.8em);">
    <h2 style="margin:0;">2019</h2>
  </div>
</div>

<!-- Talk 1 -->
<div style="display:flex; align-items:flex-start; gap:0.8em; margin-bottom:2em;">
  <!-- Optional: placeholder square or icon -->
    <img src="/assets/images/publications/inbetween-uncertainty-banner.png"
     alt="Icon"
     style="width: 50px; height: 50px; object-fit: cover; border-radius: 6px; flex-shrink: 0; margin-top: 6px;">

  <div>
    <h3 style="margin-top: 0em; margin-bottom: 0.1em; text-indent: -0.47em;">
      “In-Between” Uncertainty in Bayesian Neural Networks
    </h3>

    <span class="small-caps">
      contributed talk, icml workshop on uncertainty in deep learning
    </span>

    <p style="margin-top: 0.6em;">
      A contributed talk explaining my <a href="https://arxiv.org/abs/1906.11537" target="_blank" rel="noopener noreferrer">paper</a> on the lack of “in-between” uncertainty when using the mean-field approximation in Bayesian neural networks. Watch the <a href="https://www.facebook.com/icml.imls/videos/320132412242165/?t=1720" target="_blank" rel="noopener noreferrer">video</a> (beginning at 28:30), or view the <a href="../files/ICML_2019_Workshop_Presentation.pdf" target="_blank">slides</a>.
    </p>
  </div>
</div>

<!-- Talk 2 -->
<div style="display:flex; align-items:flex-start; gap:0.8em; margin-bottom:2em;">
    <img src="/assets/images/talks/implicit.png"
     alt="Icon"
     style="width: 50px; height: 50px; object-fit: cover; border-radius: 6px; flex-shrink: 0; margin-top: 6px;">

  <div>
    <h3 style="margin-top: 0em; margin-bottom: 0.1em;">
      Implicit Variational Inference
    </h3>

    <span class="small-caps">
      cambridge machine learning reading group
    </span>

    <p style="margin-top: 0.6em;">
      I gave a talk with <a href="https://davidrburt.github.io/" target="_blank" rel="noopener noreferrer">David Burt</a> introducing implicit variational inference, a way to obtain very flexible approximate posterior distributions for Bayesian inference. View the <a href="../files/Implicit_Inference_RG_notes.pdf" target="_blank">slides</a>.
    </p>
  </div>
</div>

<hr class="hr-ghost">

<a id="blog" style="display: block; position: relative; top: -50px; visibility: hidden;"></a>
<h1>Blog Posts</h1>

*Occasional writings and tutorials on machine learning.*

<!-- Blog 1: Neural Process Family -->
<div style="display:flex; align-items:flex-start; gap:0.8em; margin-bottom:2em;">

  <img src="/assets/images/blogs/npf-icon.png"
       alt="Neural Process Family icon"
       style="width:50px; height:50px; object-fit:cover; border-radius:6px; flex-shrink:0; margin-top: 6px;">

  <div>
    <h3 style="margin-top: 0em; margin-bottom: 1em;">
      <a href="https://yanndubs.github.io/Neural-Process-Family/text/Intro.html" target="_blank" rel="noopener noreferrer">
        The Neural Process Family
      </a>
    </h3>

    <p style="margin-bottom:0; margin-top:-0.8em; font-size: 1.0rem">
      Yann Dubois, Jonathan Gordon, <strong>Andrew Y. K. Foong</strong>
    </p>

    <p style="margin-top:0.4em;">
      Deep learning shines when there's lots of data, but fails in low-data settings where uncertainty matters, commonly encountered in medical time-series. The <span class="small-caps">npf</span> is a family of models that tackles this by meta-learning a distribution over predictors, blending stochastic processes with neural networks. This site walks through the ideas, maths, and code from scratch.
    </p>
  </div>
</div>

<!-- Blog 2: Bayesian Posts -->
<div style="display:flex; align-items:flex-start; gap:0.8em; margin-bottom:2em;">

  <img src="/assets/images/blogs/bayesian-icon.png"
       alt="Bayesian blog icon"
       style="width:50px; height:50px; object-fit:cover; border-radius:6px; flex-shrink:0; margin-top: 6px;">

  <div>
    <h3 style="margin-top: 0em; margin-bottom: 1em;">
      <a href="https://mlg.eng.cam.ac.uk/blog/2021/03/31/what-keeps-a-bayesian-awake-at-night-part-1.html" target="_blank" rel="noopener noreferrer">
        What Keeps a Bayesian Awake at Night?
      </a>
    </h3>

    <p style="margin-bottom:0; margin-top:-0.8em; font-size: 1.0rem">
      Wessel P. Bruinsma, <strong>Andrew Y. K. Foong</strong>, Richard E. Turner
    </p>

    <p style="margin-top:0.4em;">
      A two-part blog exploring the sunny arguments <span style="font-style: italic">for</span> Bayesian inference, and then, in part two, taking a critical perspective. Written by members of an avowedly Bayesian Cambridge research group, we look at where the theory breaks, the modelling compromises we make, and the computational hurdles that keep us up at night.
      <br>

      <a href="https://mlg.eng.cam.ac.uk/blog/2021/03/31/what-keeps-a-bayesian-awake-at-night-part-1.html" target="_blank" rel="noopener noreferrer">
      <span class="small-caps">part 1: day</span>
      </a>
      &nbsp;&nbsp;<strong>|</strong>&nbsp;&nbsp;
      <a href="https://mlg.eng.cam.ac.uk/blog/2021/03/31/what-keeps-a-bayesian-awake-at-night-part-2.html" target="_blank" rel="noopener noreferrer">
      <span class="small-caps">part 2: night</span>
      </a>


    </p>
  </div>
</div>


<hr class="hr-ghost">

<a id="cv" style="display: block; position: relative; top: -50px; visibility: hidden;"></a>
<h1>Curriculum Vitae</h1>

<p>
  My academic <span class="small-caps">cv</span> is available as a <span class="small-caps">pdf</span> 
  <a href="/files/CV.pdf" target="_blank" rel="noopener noreferrer">here</a>.
  <br>
  <span style="font-style: italic; color: #666; font-size: 1.0rem;">Last updated January 2025.</span>
</p>