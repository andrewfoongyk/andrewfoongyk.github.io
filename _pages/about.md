---
permalink: /
title: null #"About Me"
excerpt: "About me"
author_profile: true
redirect_from:
  - /about/
  - /about.html
---

<style>
  .page__title {
    display: none;
  }
</style>

<a id="about" style="display: block; position: relative; top: -200px; visibility: hidden;"></a>
<!-- <h1 style = "margin-top: -1.5rem;">About Me</h1> -->
<h1 class="about-heading">About Me</h1>

I am an
<span class="small-caps">ai</span>
scientist at the 
Mayo Clinic,
<!-- <a href="https://www.mayoclinic.org">Mayo Clinic</a>,  -->
where I lead the
<span class="small-caps">ai</span>
and Data Analytics 
<span class="small-caps">(aida)</span>
team in the 
Department of Radiation Oncology.
<!-- [Department of Radiation Oncology](https://www.mayoclinic.org/departments-centers/radiation-oncology/home/orc-20188588). -->
We develop next-generation 
<span class="small-caps">ai</span>
systems to advance cancer treatment.
See my [faculty page](https://www.mayo.edu/research/faculty/foong-andrew-ph-d/bio-20583559) for research areas.

Before joining Mayo, I was a senior researcher
at
Microsoft Research in Cambridge,
<!-- <a href= "https://www.microsoft.com/en-us/research/lab/microsoft-research-cambridge/">Microsoft Research</a> in Cambridge, -->
<span class="small-caps">uk</span>,
where I developed generative deep learning models for proteins in the 
<span class="small-caps">ai</span> for Science team.
<!-- [<span class="small-caps">ai</span> for Science team](https://www.microsoft.com/en-us/research/lab/microsoft-research-ai4science/). -->
Our work on
<a href="https://www.microsoft.com/en-us/research/video/scalable-emulation-of-protein-equilibrium-ensembles-with-bioemu/" >BioEmu</a>
was published in the journal [*Science*](https://www.science.org/doi/10.1126/science.adv9817).

I earned my 
<span class="small-caps">p</span>h.<span class="small-caps">d</span>.
in machine learning
at the
University of Cambridge, advised by 
Professor Richard E. Turner.
<!-- [Professor Richard E. Turner](https://rich-turner-group.github.io). -->
My research, combining probabilistic modeling and deep learning, was published at leading machine learning conferences including
<span class="small-caps">n</span>eur<span class="small-caps">ips, iclr</span>
<!-- <span class="small-caps">neurips, iclr</span> -->
and
<span class="small-caps">icml</span>.

See my selected [papers](#publications), or visit my Google Scholar [profile](https://scholar.google.com/citations?user=2UOjgIUAAAAJ&hl=en) for a full list.

<span class="small-caps"> contact</span>:<span class="email">
foong.andrew@mayo.edu
<!-- foong [dot] andrew [at] mayo [dot] edu -->
</span>


<h1>Hiring News</h1>
I am hiring for research fellow positions in <span class="small-caps">ai</span> for cancer treatment in the Mayo Clinic
<span class="small-caps">aida</span>
team. Apply [here](https://jobs.mayoclinic.org/job/rochester/research-fellow-ai-and-data-analytics-aida-radiation-oncology/33647/80077797392) or reach out via [email](mailto:foong.andrew@mayo.edu).

<h1>Experience</h1>

<div style="display: flex; align-items: flex-start; margin-bottom: 1em;">
  <img src="../assets/mayo_clinic_logo.jpeg" alt="Mayo Clinic logo"
       style="width: 50px; height: 50px; margin-right: 15px; flex-shrink: 0; vertical-align: top; margin-top: 10px;" />
  <div style="margin-top: 0;">
    <p style="margin-top: 0;">
      <span class="small-caps">ai scientist, senior associate consultant, mayo clinic</span><br/>
      <em>January 2025–present, Rochester, Minnesota, <span class="small-caps">usa</span></em><br/>
      Department of Radiation Oncology<br/>
      <em>Topics</em>: Deep learning for cancer treatment
    </p>
  </div>
</div>

<div style="display: flex; align-items: flex-start; margin-bottom: 1em;">
  <img src="../assets/msr.jpeg" alt="Microsoft logo"
       style="width: 50px; height: 50px; margin-right: 15px; flex-shrink: 0; vertical-align: top; margin-top: 10px;" />
  <div style="margin-top: 0;">
    <p style="margin-top: 0;">
      <span class="small-caps">senior researcher, microsoft research</span><br/>
      <em>November 2022–November 2024, Cambridge, <span class="small-caps">uk</span></em><br/>
      <span class="small-caps">ai</span> for Science<br/>
      <em>Topics</em>: Deep learning for protein generative modeling
    </p>
  </div>
</div>

<div style="display: flex; align-items: flex-start; margin-bottom: 1em;">
  <img src="../assets/camlogo.png" alt="University of Cambridge logo"
       style="width: 50px; height: 50px; margin-right: 15px; flex-shrink: 0; vertical-align: top; margin-top: 10px;" />
  <div style="margin-top: 0;">
    <p style="margin-top: 0;">
      <span class="small-caps">ph.d. in machine learning, university of cambridge</span><br/>
      <em>October 2018–November 2022, Cambridge, <span class="small-caps">uk</span></em><br/>
      Computational and Biological Learning Lab <span class="small-caps">(cbl)</span><br/>
      <em>Supervisor</em>: Professor Richard E. Turner<br/>
     <em>Topics</em>: Deep learning, probabilistic inference
    </p>
  </div>
</div>

<div style="display: flex; align-items: flex-start; margin-bottom: 1em;">
  <img src="../assets/deepmind.png" alt="DeepMind logo"
       style="width: 50px; height: 50px; margin-right: 15px; flex-shrink: 0; vertical-align: top; margin-top: 10px;" />
  <div style="margin-top: 0;">
    <p style="margin-top: 0;">
      <span class="small-caps">research scientist intern, google deepmind</span><br/>
      <em>February 2022–May 2022, London, <span class="small-caps">uk</span></em><br/>
      <em>Supervisor</em>: Dr. Michalis Titsias<br/>
      <em>Topics</em>: Deep generative modeling, Gaussian processes
    </p>
  </div>
</div>

<div style="display: flex; align-items: flex-start; margin-bottom: 1em;">
  <img src="../assets/msr.jpeg" alt="Microsoft logo"
       style="width: 50px; height: 50px; margin-right: 15px; flex-shrink: 0; vertical-align: top; margin-top: 10px;" />
  <div style="margin-top: 0;">
    <p style="margin-top: 0;">
      <span class="small-caps">research intern, microsoft research cambridge</span><br/>
      <em>July 2021–October 2021, Cambridge, <span class="small-caps">uk</span></em><br/>
      <em>Supervisor</em>: Dr. Sebastian Nowozin<br/>
      <em>Topics</em>: Deep learning for simulating protein dynamics
    </p>
  </div>
</div>

<div style="display: flex; align-items: flex-start; margin-bottom: 1em;">
  <img src="../assets/camlogo.png" alt="University of Cambridge logo"
       style="width: 50px; height: 50px; margin-right: 15px; flex-shrink: 0; vertical-align: top; margin-top: 10px;" />
  <div style="margin-top: 0;">
    <p style="margin-top: 0;">
      <span class="small-caps">b.a. & m.eng. in information and computer engineering, university of cambridge</span><br/>
      <em>October 2014–July 2018, Cambridge, <span class="small-caps">uk</span></em><br/>
      Department of Engineering<br/>
      First class honours with distinction, scored top 1–2% in year group
    </p>
  </div>
</div>

<a id="publications" style="display: block; position: relative; top: -58px; visibility: hidden;"></a>
<h1>Research Papers</h1>

<div style="display:flex; align-items:flex-start; gap:1.2em; margin-bottom:0.5em;">
    <p style="margin:0; font-style: italic;">
      For a full list of research papers, see my
      <a href="https://scholar.google.com/citations?user=2UOjgIUAAAAJ&hl=en" target="_blank">
        Google Scholar page</a>. 
      Asterisks denote co-first authorship.
    </p>
</div>


<div style="display:flex; align-items:flex-start; gap:1.2em; margin-bottom:0.5em;">
  <div style="margin-left: calc(50px + 0.8em);">
    <h2 style="margin:0;">2024</h2>
  </div>
</div>

<div style="display:flex; align-items:flex-start; gap:0.8em; margin-bottom:2em;">

  <!-- Square thumbnail -->
  <img src="/assets/images/publications/bioemu-banner.png"
       alt="BioEmu figure"
       style="width:50px; height:50px; object-fit:cover; border-radius:1px; flex-shrink:0;">

  <!-- Text content -->
  <div>
    <!-- Remove default top margin so it lines up with the image -->
    <h3 style="margin:0 0 0.0em 0;">
      <a href="https://www.biorxiv.org/content/10.1101/2024.12.05.626885">
        Scalable Emulation of Protein Equilibrium Ensembles With Generative Deep Learning
      </a>
    </h3>

    <p style="margin:0 0 0.0em; font-style:italic;">
      Science
    </p>

    <p style="margin-bottom:0; margin-top:-0.8em; font-size:0.9rem">
      Sarah Lewis*, Tim Hempel*, José Jiménez-Luna*, Michael Gastegger*, Yu Xie*, <strong>Andrew Y. K. Foong</strong>*, Victor García Satorras*, Osama Abdin*, Bastiaan S. Veeling*, Iryna Zaporozhets, Yaoyi Chen, Soojung Yang, Arne Schneuing, Jigyasa Nigam, Federico Barbero, Vincent Stimper, Andrew Campbell, Jason Yim, Marten Lienen, Yu Shi, Shuxin Zheng, Hannes Schulz, Usman Munir, Cecilia Clementi, Frank Noé
    </p>

    <details style="margin-top:0.0em;">
      <summary style="cursor:pointer; margin-top:-0.8em; margin-top:-0.8em; font-variant: small-caps;">abstract</summary>
      <p style="margin-top:0.4em;">
       Following the sequence and structure revolutions, predicting the dynamical mechanisms of proteins that implement biological function remains an outstanding scientific challenge. Several experimental techniques and molecular dynamics <span class="small-caps">(md)</span> simulations can, in principle, determine conformational states, binding configurations and their probabilities, but suffer from low throughput. Here we develop a Biomolecular Emulator (BioEmu), a generative deep learning system that can generate thousands of statistically independent samples from the protein structure ensemble per hour on a single graphical processing unit. By leveraging novel training methods and vast data of protein structures, over 200 milliseconds of <span class="small-caps">md</span> simulation, and experimental protein stabilities, BioEmu’s protein ensembles represent equilibrium in a range of challenging and practically relevant metrics. Qualitatively, BioEmu samples many functionally relevant conformational changes, ranging from formation of cryptic pockets, over unfolding of specific protein regions, to large-scale domain rearrangements. Quantitatively, BioEmu samples protein conformations with relative free energy errors around 1 kcal/mol, as validated against millisecond-timescale <span class="small-caps">md</span> simulation and experimentally-measured protein stabilities. By simultaneously emulating structural ensembles and thermodynamic properties, BioEmu reveals mechanistic insights, such as the causes for fold destabilization of mutants, and can efficiently provide experimentally-testable hypotheses.
      </p>
    </details>
  </div>
</div>

<div style="display:flex; align-items:flex-start; gap:1.2em; margin-bottom:0.5em;">
  <div style="margin-left: calc(50px + 0.8em);">
    <h2 style="margin:0;">2023</h2>
  </div>
</div>

<div style="display:flex; align-items:flex-start; gap:0.8em; margin-bottom:2em;">

  <!-- Square thumbnail -->
  <img src="/assets/images/publications/frameflow-banner.png"
       alt="FrameFlow figure"
       style="width:50px; height:50px; object-fit:cover; border-radius:6px; flex-shrink:0;">

  <!-- Text content -->
  <div>
    <h3 style="margin:0 0 0.3em 0;">
      <a href="https://www.mlsb.io/papers_2023/Fast_protein_backbone_generation_with_SE3_flow_matching.pdf">
        Fast Protein Backbone Generation with SE(3) Flow Matching
      </a>
    </h3>

    <p style="margin:0 0 0.3em; font-style:italic;">
      <span class="small-caps">n</span>eur<span class="small-caps">ips</span> Machine Learning for Structural Biology workshop <span class="small-caps">(mlsb)</span>
    </p>

    <p style="margin-bottom:0; margin-top:-0.8em; font-size: 0.9rem">
      Jason Yim, Andrew Campbell, <strong>Andrew Y. K. Foong</strong>, Michael Gastegger,
      José Jiménez-Luna, Sarah Lewis, Victor Garcia Satorras, Bastiaan S. Veeling,
      Regina Barzilay, Tommi Jaakkola, Frank Noé
    </p>

    <details style="margin-top:0.2em;">
      <summary style="cursor:pointer; margin-top:-0.8em; margin-top:-0.8em; font-variant: small-caps;">abstract</summary>
      <p style="margin-top:0.4em;">
        We present FrameFlow, a method for fast protein backbone generation using <span class="small-caps">se(</span>3<span class="small-caps">)</span> flow matching. Specifically, we adapt FrameDiff, a state-of-the-art diffusion model, to the flow-matching generative modeling paradigm. We show how flow matching can be applied on <span class="small-caps">se(</span>3<span class="small-caps">)</span> and propose modifications during training to effectively learn the vector field. Compared to FrameDiff, FrameFlow requires five times fewer sampling timesteps while achieving two fold better designability. The ability to generate high quality protein samples at a fraction of the cost of previous methods paves the way towards more efficient generative models in de novo protein design.
      </p>
    </details>
  </div>
</div>


<!-- Paper -->
<div style="display:flex; align-items:flex-start; gap:0.8em; margin-bottom:2em;">

  <!-- Square thumbnail -->
  <img src="/assets/images/publications/acnp-banner.png"
       alt="Autoregressive Conditional Neural Processes figure"
       style="width:50px; height:50px; object-fit:cover; border-radius:6px; flex-shrink:0;">

  <!-- Text content -->
  <div>
    <h3 style="margin:0 0 0.3em 0;">
      <a href="https://openreview.net/forum?id=OAsXFPBfTBh">
        Autoregressive Conditional Neural Processes
      </a>
    </h3>

    <p style="margin:0 0 0.3em; font-style:italic;">
      International Conference on Learning Representations <span class="small-caps">(iclr)</span>
    </p>

    <p style="margin-bottom:0; margin-top:-0.8em; font-size: 0.9rem">
      Wessel Bruinsma*, Stratis Markou*, James Requeima*, <strong>Andrew Y. K. Foong</strong>*,
      Anna Vaughan, Tom Andersson, Anthony Buonomo, Scott Hosking, Richard E. Turner
    </p>

    <details style="margin-top:0.2em;">
      <summary style="cursor:pointer; margin-top:-0.8em; font-variant: small-caps;">abstract</summary>
      <p style="margin-top:0.4em;">
        Conditional neural processes (<span class="small-caps">cnp</span>s; Garnelo et al., 2018a) are attractive meta-learning models which produce well-calibrated predictions and are trainable via a simple maximum likelihood procedure. Although <span class="small-caps">cnp</span>s have many advantages, they are unable to model dependencies in their predictions. Various works propose solutions to this, but these come at the cost of either requiring approximate inference or being limited to Gaussian predictions. In this work, we instead propose to change how <span class="small-caps">cnp</span>s are deployed at test time, without any modifications to the model or training procedure. Instead of making predictions independently for every target point, we autoregressively define a joint predictive distribution using the chain rule of probability, taking inspiration from the neural autoregressive density estimator <span class="small-caps">(nade)</span> literature. We show that this simple procedure allows factorised Gaussian <span class="small-caps">cnp</span>s to model highly dependent, non-Gaussian predictive distributions. Perhaps surprisingly, in an extensive range of tasks with synthetic and real data, we show that <span class="small-caps">cnp</span>s in autoregressive <span class="small-caps">(ar)</span> mode not only significantly outperform non-<span class="small-caps">ar</span> <span class="small-caps">cnp</span>s, but are also competitive with more sophisticated models that are significantly more computationally expensive and challenging to train.
      </p>
    </details>
  </div>
</div>


<!-- Paper -->
<div style="display:flex; align-items:flex-start; gap:0.8em; margin-bottom:2em;">

  <!-- Square thumbnail -->
  <img src="/assets/images/publications/timewarp-banner.png"
       alt="Timewarp figure"
       style="width:50px; height:50px; object-fit:cover; border-radius:6px; flex-shrink:0;">

  <!-- Text content -->
  <div>
    <h3 style="margin:0 0 0.3em 0;">
      <a href="https://arxiv.org/abs/2302.01170">
        Timewarp: Transferable Acceleration of Molecular Dynamics by Learning Time-Coarsened Dynamics
      </a>
    </h3>

    <p style="margin:0 0 0.3em; font-style:italic;">
      Neural Information Processing Systems <span class="small-caps">(n</span>eur<span class="small-caps">ips)</span> 2023 (spotlight presentation)
    </p>

    <p style="margin-bottom:0; margin-top:-0.8em; font-size: 0.9rem">
      Leon Klein*, <strong>Andrew Y. K. Foong</strong>*, Tor Erlend Fjelde*, Bruno Mlodozeniec*,
      Marc Brockschmidt, Sebastian Nowozin, Frank Noé, Ryota Tomioka
    </p>

    <details style="margin-top:0.2em;">
      <summary style="cursor:pointer; margin-top:-0.8em; font-variant: small-caps;">abstract</summary>
      <p style="margin-top:0.4em;">
        Molecular dynamics <span class="small-caps">(md)</span> simulation is a widely used technique to simulate molecular systems, most commonly at the all-atom resolution where the equations of motion are integrated with timesteps on the order of femtoseconds (1&nbsp;fs = 10<sup>−15</sup>&nbsp;s). <span class="small-caps">md</span> is often used to compute equilibrium properties, which requires sampling from an equilibrium distribution such as the Boltzmann distribution. However, many important processes, such as binding and folding, occur over timescales of milliseconds or beyond, and cannot be efficiently sampled with conventional <span class="small-caps">md</span>. Furthermore, new <span class="small-caps">md</span> simulations need to be performed from scratch for each molecular system studied. We present Timewarp, an enhanced sampling method which uses a normalising flow as a proposal distribution in a Markov chain Monte Carlo method targeting the Boltzmann distribution. The flow is trained offline on <span class="small-caps">md</span> trajectories and learns to make large steps in time, simulating the molecular dynamics of 10<sup>5</sup>−10<sup>6</sup>&nbsp;fs. Crucially, Timewarp is transferable between molecular systems: once trained, we show that it generalises to unseen small peptides (2–4 amino acids), exploring their metastable states and providing wall-clock acceleration when sampling compared to standard <span class="small-caps">md</span>. Our method constitutes an important step towards developing general, transferable algorithms for accelerating <span class="small-caps">md</span>.
      </p>
    </details>
  </div>
</div>

<div style="display:flex; align-items:flex-start; gap:1.2em; margin-bottom:0.5em;">
  <div style="margin-left: calc(50px + 0.8em);">
    <h2 style="margin:0;">2022</h2>
  </div>
</div>


<!-- Paper -->
<div style="display:flex; align-items:flex-start; gap:0.8em; margin-bottom:2em;">

  <!-- Square thumbnail -->
  <img src="/assets/images/publications/thesis-banner.png"
       alt="Thesis figure"
       style="width:50px; height:50px; object-fit:cover; border-radius:6px; flex-shrink:0;">

  <!-- Text content -->
  <div>
    <h3 style="margin:0 0 0.3em 0;">
      <a href="https://www.repository.cam.ac.uk/items/bb696578-0b93-44da-82e9-4896a1ab9266">
        Approximate Inference in Bayesian Neural Networks and Translation Equivariant Neural Processes
      </a>
    </h3>

    <p style="margin:0 0 0.3em; font-style:italic;">
      <span class="small-caps">p</span>h.<span class="small-caps">d</span>. thesis, University of Cambridge 
    </p>

    <p style="margin-bottom:0; margin-top:-0.8em; font-size: 0.9rem">
      <strong>Andrew Y. K. Foong</strong>
    </p>

    <details style="margin-top:0.2em;">
      <summary style="cursor:pointer; margin-top:-0.8em; font-variant: small-caps;">abstract</summary>
      <p style="margin-top:0.4em;">
       It has been a longstanding goal in machine learning to develop flexible prediction methods that ‘know what they don’t know’—when faced with an out-of-distribution input, these models should signal their uncertainty rather than be confidently wrong. This thesis is concerned with two such probabilistic machine learning models: Bayesian neural networks and neural processes. Bayesian neural networks are a classical model that has been the subject of research since the 1990s. They rely on Bayesian inference to represent uncertainty in the weights of a neural network. On the other hand, neural processes are a recently introduced model that relies on meta-learning rather than Bayesian inference to obtain uncertainty estimates.

       This thesis provides contributions to both of these research areas. For Bayesian neural networks, we provide a theoretical and empirical study of the quality of common variational methods in approximating the Bayesian predictive distribution. We show that for single-hidden layer networks with <span class="small-caps">r</span>e<span class="small-caps">lu</span> activation functions, there are fundamental limitations concerning the representation of in-between uncertainty: increased uncertainty in between well separated regions of low uncertainty. We show that this theoretical limitation doesn’t apply for deeper networks. However, in practice, in-between uncertainty is a feature of the exact predictive distribution that is still often lost by approximate inference, even with deep networks.

       In the second part of this thesis, we focus on neural processes. In contrast to Bayesian neural networks, neural processes do not rely on approximate inference. Instead, they use neural networks to directly parameterise the map from a dataset to the posterior predictive stochastic process conditioned on that dataset. In this thesis we introduce the convolutional neural process, a new kind of neural process architecture which incorporates translation equivariance into its predictions. We show that when this symmetry is an appropriate assumption, convolutional neural processes outperform their standard multilayer perceptron-based and attentive counterparts on a variety of regression benchmarks. 
      </p>
    </details>
  </div>
</div>

<div style="display:flex; align-items:flex-start; gap:1.2em; margin-bottom:0.5em;">
  <div style="margin-left: calc(50px + 0.8em);">
    <h2 style="margin:0;">2021</h2>
  </div>
</div>


<!-- Paper -->
<div style="display:flex; align-items:flex-start; gap:0.8em; margin-bottom:2em;">

  <!-- Square thumbnail -->
  <img src="/assets/images/publications/bnn-collapsed-banner.png"
       alt="Collapsed Variational Bounds for bnns figure"
       style="width:50px; height:50px; object-fit:cover; border-radius:6px; flex-shrink:0;">

  <!-- Text content -->
  <div>
    <h3 style="margin:0 0 0.3em 0;">
      <a href="https://openreview.net/forum?id=ykN3tbJ0qmX">
        Collapsed Variational Bounds for Bayesian Neural Networks
      </a>
    </h3>

    <p style="margin:0 0 0.3em; font-style:italic;">
      Neural Information Processing Systems <span class="small-caps">(n</span>eur<span class="small-caps">ips)</span>
    </p>

    <p style="margin-bottom:0; margin-top:-0.8em; font-size: 0.9rem">
      Marcin B. Tomczak, Siddharth Swaroop, <strong>Andrew Y. K. Foong</strong>, Richard E. Turner
    </p>

    <details style="margin-top:0.2em;">
      <summary style="cursor:pointer; margin-top:-0.8em; font-variant: small-caps;">abstract</summary>
      <p style="margin-top:0.4em;">
        Recent interest in learning large variational Bayesian Neural Networks <span class="small-caps">(bnn</span>s<span class="small-caps">)</span>  has been partly hampered by poor predictive performance caused by underfitting, and their performance is known to be very sensitive to the prior over weights. Current practice often fixes the prior parameters to standard values or tunes them using heuristics or cross-validation. In this paper, we treat prior parameters in a distributional way by extending the model and collapsing the variational bound with respect to their posteriors. This leads to novel and tighter Evidence Lower Bounds <span class="small-caps">(elbo</span>s<span class="small-caps">)</span> for performing variational inference <span class="small-caps">(vi)</span> in <span class="small-caps">bnn</span>s. Our experiments show that the new bounds significantly improve the performance of Gaussian mean-field <span class="small-caps">vi</span> applied to <span class="small-caps">bnn</span>s on a variety of data sets, demonstrating that mean-field <span class="small-caps">vi</span> works well even in deep models. We also find that the tighter <span class="small-caps">elbo</span>s can be good optimization targets for learning the hyperparameters of hierarchical priors.
      </p>
    </details>
  </div>
</div>

<!-- Paper -->
<div style="display:flex; align-items:flex-start; gap:0.8em; margin-bottom:2em;">

  <!-- Square thumbnail -->
  <img src="/assets/images/publications/pacbayes-smalldata-banner.png"
       alt="pac-Bayes Small Data figure"
       style="width:50px; height:50px; object-fit:cover; border-radius:6px; flex-shrink:0;">

  <!-- Text content -->
  <div>
    <h3 style="margin:0 0 0.3em 0;">
      <a href="https://arxiv.org/abs/2106.03542">
        How Tight Can PAC-Bayes be in the Small Data Regime?
      </a>
    </h3>

    <p style="margin:0 0 0.3em; font-style:italic;">
      Neural Information Processing Systems <span class="small-caps">(n</span>eur<span class="small-caps">ips)</span>
    </p>    

    <p style="margin-bottom:0; margin-top:-0.8em; font-size: 0.9rem">
      <strong>Andrew Y. K. Foong</strong>*, Wessel P. Bruinsma*, David R. Burt, and Richard E. Turner
    </p>

    <details style="margin-top:0.2em;">
      <summary style="cursor:pointer; margin-top:-0.8em; font-variant: small-caps;">abstract</summary>
      <p style="margin-top:0.4em;">
        In this paper, we investigate the question: Given a small number of datapoints, for example <span class="small-caps">n</span> = 30, how tight can <span class="small-caps">pac</span>-Bayes and test set bounds be made? For such small datasets, test set bounds adversely affect generalisation performance by discarding data. In this setting, <span class="small-caps">pac</span>-Bayes bounds are especially attractive, due to their ability to use all the data to simultaneously learn a posterior and bound its generalisation risk. We focus on the case of i.i.d. data with a bounded loss and consider the generic <span class="small-caps">pac</span>-Bayes theorem of Germain et al. (2009) and Begin et al. (2016). While their theorem is known to recover many existing <span class="small-caps">pac</span>-Bayes bounds, it is unclear what the tightest bound derivable from their framework is. Surprisingly, we show that for a fixed learning algorithm and dataset, the tightest bound of this form coincides with the tightest bound of the more restrictive family of bounds considered in Catoni (2007). In contrast, in the more natural case of distributions over datasets, we give examples (both analytic and numerical) showing that the family of bounds in Catoni (2007) can be suboptimal. Within the proof framework of Germain et al. (2009) and Begin et al. (2016), we establish a lower bound on the best bound achievable in expectation, which recovers the Chernoff test set bound in the case when the posterior is equal to the prior. Finally, to illustrate how tight these bounds can potentially be, we study a synthetic one-dimensional classification task in which it is feasible to meta-learn both the prior and the form of the bound to obtain the tightest <span class="small-caps">pac</span>-Bayes and test set bounds possible. We find that in this simple, controlled scenario, <span class="small-caps">pac</span>-Bayes bounds are surprisingly competitive with comparable, commonly used Chernoff test set bounds. However, the sharpest test set bounds still lead to better guarantees on the generalisation error than the <span class="small-caps">pac</span>-Bayes bounds we consider.
      </p>
    </details>
  </div>
</div>


<!-- Paper -->
<div style="display:flex; align-items:flex-start; gap:0.8em; margin-bottom:2em;">

  <!-- Square thumbnail -->
  <img src="/assets/images/publications/gnp-banner.png"
       alt="Gaussian Neural Process figure"
       style="width:50px; height:50px; object-fit:cover; border-radius:6px; flex-shrink:0;">

  <!-- Text content -->
  <div>
    <h3 style="margin:0 0 0.3em 0;">
      <a href="https://arxiv.org/abs/2101.03606">
        The Gaussian Neural Process
      </a>
    </h3>

    <p style="margin:0 0 0.3em; font-style:italic;">
      Advances in Approximate Bayesian Inference <span class="small-caps">(aabi)</span>
    </p>    

    <p style="margin-bottom:0; margin-top:-0.8em; font-size: 0.9rem">
      Wessel P. Bruinsma, James Requeima, <strong>Andrew Y. K. Foong</strong>,
      Jonathan Gordon, and Richard E. Turner
    </p>

    <details style="margin-top:0.2em;">
      <summary style="cursor:pointer; margin-top:-0.8em; font-variant: small-caps;">abstract</summary>
      <p style="margin-top:0.4em;">
        Neural Processes (<span class="small-caps">np</span>s; Garnelo et al., 2018a,b) are a rich class of models for meta-learning that map data sets directly to predictive stochastic processes. We provide a rigorous analysis of the standard maximum-likelihood objective used to train conditional <span class="small-caps">np</span>s. Moreover, we propose a new member to the Neural Process family called the Gaussian Neural Process <span class="small-caps">(gnp)</span>, which models predictive correlations, incorporates translation equivariance, provides universal approximation guarantees, and demonstrates encouraging performance.
      </p>
    </details>
  </div>
</div>



<div style="display:flex; align-items:flex-start; gap:1.2em; margin-bottom:0.5em;">
  <div style="margin-left: calc(50px + 0.8em);">
    <h2 style="margin:0;">2020</h2>
  </div>
</div>


<div style="display:flex; align-items:flex-start; gap:0.8em; margin-bottom:2em;">

  <!-- Square thumbnail -->
  <img src="/assets/images/publications/convnp-banner.png"
       alt="ConvNP figure"
       style="width:50px; height:50px; object-fit:cover; border-radius:6px; flex-shrink:0;">

  <!-- Text content -->
  <div>
    <h3 style="margin:0 0 0.3em 0;">
      <a href="https://arxiv.org/abs/2007.01332">
        Meta-Learning Stationary Stochastic Process Prediction with Convolutional Neural Processes
      </a>
    </h3>

    <p style="margin:0 0 0.3em; font-style:italic;">
      Neural Information Processing Systems <span class="small-caps">(n</span>eur<span class="small-caps">ips)</span>
    </p>

    <p style="margin-bottom:0; margin-top:-0.8em; font-size: 0.9rem">
      <strong>Andrew Y. K. Foong</strong>*, Wessel P. Bruinsma*, Jonathan Gordon*, Yann Dubois,
      James Requeima, and Richard E. Turner
    </p>

    <details style="margin-top:0.2em;">
      <summary style="cursor:pointer; margin-top:-0.8em; font-variant: small-caps;">abstract</summary>
      <p style="margin-top:0.4em;">
        Stationary stochastic processes <span class="small-caps">(sp</span>s<span class="small-caps">)</span> are a key component of many probabilistic models, such as those for off-the-grid spatio-temporal data. They enable the statistical symmetry of underlying physical phenomena to be leveraged, thereby aiding generalization. Prediction in such models can be viewed as a translation equivariant map from observed data sets to predictive <span class="small-caps">sp</span>s, emphasizing the intimate relationship between stationarity and equivariance. Building on this, we propose the Convolutional Neural Process (Conv<span class="small-caps">np</span>), which endows Neural Processes <span class="small-caps">(np</span>s<span class="small-caps">)</span> with translation equivariance and extends convolutional conditional <span class="small-caps">np</span>s to allow for dependencies in the predictive distribution. The latter enables Conv<span class="small-caps">np</span>s to be deployed in settings which require coherent samples, such as Thompson sampling or conditional image completion. Moreover, we propose a new maximum-likelihood objective to replace the standard <span class="small-caps">elbo</span> objective in <span class="small-caps">np</span>s, which conceptually simplifies the framework and empirically improves performance. We demonstrate the strong performance and generalization capabilities of Conv<span class="small-caps">np</span>s on 1<span class="small-caps">d</span> regression, image completion, and various tasks with real-world spatio-temporal data.
      </p>
    </details>
  </div>
</div>


<!-- Paper -->
<div style="display:flex; align-items:flex-start; gap:0.8em; margin-bottom:2em;">

  <!-- Square thumbnail -->
  <img src="/assets/images/publications/convcnp-banner.png"
       alt="Convolutional Conditional Neural Processes figure"
       style="width:50px; height:50px; object-fit:cover; border-radius:6px; flex-shrink:0;">

  <!-- Text content -->
  <div>
    <h3 style="margin:0 0 0.3em 0;">
      <a href="https://arxiv.org/abs/1910.13556">
        Convolutional Conditional Neural Processes
      </a>
    </h3>

    <p style="margin:0 0 0.3em; font-style:italic;">
      International Conference on Learning Representations (<span class="small-caps">iclr</span>)
    </p>

    <p style="margin-bottom:0; margin-top:-0.8em; font-size: 0.9rem">
      Jonathan Gordon*, Wessel P. Bruinsma*, <strong>Andrew Y. K. Foong</strong>,
      James Requeima, Yann Dubois, and Richard E. Turner
    </p>

    <details style="margin-top:0.2em;">
      <summary style="cursor:pointer; margin-top:-0.8em; font-variant: small-caps;">abstract</summary>
      <p style="margin-top:0.4em;">
        We introduce the Convolutional Conditional Neural Process (Conv<span class="small-caps">cnp</span>), a new member of the Neural Process family that models translation equivariance in the data. Translation equivariance is an important inductive bias for many learning problems including time series modeling, spatial data, and images. The model embeds data sets into an infinite-dimensional function space as opposed to a finite-dimensional vector space. To formalize this notion, we extend the theory of neural representations of sets to include functional representations, and demonstrate that any translation-equivariant embedding can be represented using a convolutional deep set. We evaluate Conv<span class="small-caps">cnp</span>s in several settings, demonstrating that they achieve state-of-the-art performance compared to existing <span class="small-caps">np</span>s. We demonstrate that building in translation equivariance enables zero-shot generalization to challenging, out-of-domain tasks.
      </p>
    </details>
  </div>
</div>


<!-- Paper -->
<div style="display:flex; align-items:flex-start; gap:0.8em; margin-bottom:2em;">

  <!-- Square thumbnail -->
  <img src="/assets/images/publications/expressiveness-banner.png"
       alt="bnn figure"
       style="width:50px; height:50px; object-fit:cover; border-radius:6px; flex-shrink:0;">

  <!-- Text content -->
  <div>
    <h3 style="margin:0 0 0.3em 0;">
      <a href="https://arxiv.org/abs/1909.00719">
        On the Expressiveness of Approximate Inference in Bayesian Neural Networks
      </a>
    </h3>

    <p style="margin:0 0 0.3em; font-style:italic;">
      Neural Information Processing Systems <span class="small-caps">(n</span>eur<span class="small-caps">ips)</span>
    </p>

    <p style="margin-bottom:0; margin-top:-0.8em; font-size:0.9rem">
      <strong>Andrew Y. K. Foong</strong>*, David R. Burt*, Yingzhen Li, Richard E. Turner
    </p>

    <details style="margin-top:0.2em;">
      <summary style="cursor:pointer; margin-top:-0.8em; font-variant: small-caps;">abstract</summary>
      <p style="margin-top:0.4em;">
        While Bayesian neural networks <span class="small-caps">(bnn</span>s<span class="small-caps">)</span> hold the promise of being flexible, well-calibrated statistical models, inference often requires approximations whose consequences are poorly understood. We study the quality of common variational methods in approximating the Bayesian predictive distribution. For single-hidden layer <span class="small-caps">r</span>e<span class="small-caps">lu</span> <span class="small-caps">bnn</span>s, we prove a fundamental limitation in function-space of two of the most commonly used distributions defined in weight-space: mean-field Gaussian and Monte Carlo dropout. We find there are simple cases where neither method can have substantially increased uncertainty in between well-separated regions of low uncertainty. We provide strong empirical evidence that exact inference does not have this pathology, hence it is due to the approximation and not the model. In contrast, for deep networks, we prove a universality result showing that there exist approximate posteriors in the above classes which provide flexible uncertainty estimates. However, we find empirically that pathologies of a similar form as in the single-hidden layer case can persist when performing variational inference in deeper networks. Our results motivate careful consideration of the implications of approximate inference methods in <span class="small-caps">bnn</span>s.
      </p>
    </details>
  </div>
</div>


<div style="display:flex; align-items:flex-start; gap:1.2em; margin-bottom:0.5em;">
  <div style="margin-left: calc(50px + 0.8em);">
    <h2 style="margin:0;">2019</h2>
  </div>
</div>


<!-- Paper -->
<div style="display:flex; align-items:flex-start; gap:0.8em; margin-bottom:2em;">

  <!-- Square thumbnail -->
  <img src="/assets/images/publications/inbetween-uncertainty-banner.png"
       alt="In-Between Uncertainty in Bayesian Neural Networks figure"
       style="width:50px; height:50px; object-fit:cover; border-radius:6px; flex-shrink:0;">

  <!-- Text content -->
  <div>
    <h3 style="margin:0 0 0.3em 0;">
      <a href="https://arxiv.org/abs/1906.11537">
        <span style="display: inline-block; margin-left: -0.5em;">“</span>In-Between” Uncertainty in Bayesian Neural Networks
      </a>
    </h3>

    <p style="margin:0 0 0.3em; font-style:italic;">
      <span class="small-caps">icml</span> Workshop on Uncertainty and Robustness in Deep Learning
    </p>

    <p style="margin-bottom:0; margin-top:-0.8em; font-size: 0.9rem">
      <strong>Andrew Y. K. Foong</strong>, Yingzhen Li, José Miguel Hernández-Lobato, and Richard E. Turner
    </p>

    <details style="margin-top:0.2em;">
      <summary style="cursor:pointer; margin-top:-0.8em; font-variant: small-caps;">abstract</summary>
      <p style="margin-top:0.4em;">
        We describe a limitation in the expressiveness of the predictive uncertainty estimate given by mean-field variational inference <span class="small-caps">(mfvi)</span>, a popular approximate inference method for Bayesian neural networks. In particular, <span class="small-caps">mfvi</span> fails to give calibrated uncertainty estimates in between separated regions of observations. This can lead to catastrophically overconfident predictions when testing on out-of-distribution data. Avoiding such overconfidence is critical for active learning, Bayesian optimisation and out-of-distribution robustness. We instead find that a classical technique, the linearised Laplace approximation, can handle “in-between” uncertainty much better for small network architectures.
      </p>
    </details>
  </div>
</div>



<a id="talks" style="display: block; position: relative; top: -58px; visibility: hidden;"></a>
<h1 id="talks">Talks</h1>

<div style="display:flex; align-items:flex-start; gap:1.2em; margin-bottom:0.5em;">
    <p style="margin:0; font-style: italic;">
      A collection of my publicly available presentations and slide decks.
    </p>
</div>


<div style="display:flex; align-items:flex-start; gap:1.2em; margin-bottom:1em;">
    <h2 style="margin:0;">2025</h2>
</div>

<a id="ai-from-scratch" style="display: block; position: relative; top: -58px; visibility: hidden;"></a>
<div style="display: flex; align-items: flex-start; gap: 1.2em; margin-bottom: 0;">
  <h3 style="margin: 0;">Understanding AI from Scratch</h3>
</div>



<div style="font-variant: small-caps; margin-top: -0.1em; margin-bottom: 0.7em;">
  mayo clinic lecture series
</div>

<p>
I designed this six-part series to help clinicians at the Mayo Clinic with no prior <span class="small-caps">ai</span> experience understand deep learning from first principles. 
</p>

<ul style="margin-top: 0.2em;">
  <li>
    <span style="font-weight: 500;">Lectures 1 &amp; 2</span> cover deep learning from the ground up, from linear regression and gradient descent to neural networks, overfitting, and generalization.
  </li>
  <li>
    <span style="font-weight: 500;">Lectures 3 &amp; 4</span> explain what convolutional neural networks are and how they can be used to understand images.
  </li>
  <li>
    <span style="font-weight: 500;">Lectures 5 &amp; 6</span> explain how Chat<span class="small-caps">gpt</span> works, how it was made, and how prompting and retrieval-augmented generation <span class="small-caps">(rag)</span> can increase accuracy.
  </li>
</ul>

<div style="margin-top: 1.5em; display: flex; flex-wrap: wrap; gap: 30px; justify-content: center;">

  <div style="text-align: center; width: 220px;">
    <a href="https://mssvideoupload.mayo.edu/playlist/dedicated/1_uq8gpkab/1_rhvbtb46" target="_blank">
      <img src="/assets/lectures/lecture1-thumb.jpg" alt="Lecture 1 thumbnail" style="width: 100%; border-radius: 8px;">
    </a>
    <div style="margin-top: 0.5em; font-size: 0.95em;"><span class="small-caps">lecture 1</span>: What is Deep Learning?</div>
  </div>

  <div style="text-align: center; width: 220px;">
    <a href="https://mssvideoupload.mayo.edu/playlist/dedicated/1_uq8gpkab/1_xbkcrbjs" target="_blank">
      <img src="/assets/lectures/lecture2-thumb.jpg" alt="Lecture 2 thumbnail" style="width: 100%; border-radius: 8px;">
    </a>
    <div style="margin-top: 0.5em; font-size: 0.95em;"><span class="small-caps">lecture 2</span>: From Single Neurons to Neural Networks</div>
  </div>

  <div style="text-align: center; width: 220px;">
    <a href="https://mssvideoupload.mayo.edu/playlist/dedicated/1_uq8gpkab/1_2q24t2cy" target="_blank">
      <img src="/assets/lectures/lecture3-thumb.jpg" alt="Lecture 3 thumbnail" style="width: 100%; border-radius: 8px;">
    </a>
    <div style="margin-top: 0.5em; font-size: 0.95em;"><span class="small-caps">lecture 3</span>: <span class="small-caps">ai</span> for Imaging</div>
  </div>

  <div style="text-align: center; width: 220px;">
    <a href="https://mssvideoupload.mayo.edu/playlist/dedicated/1_uq8gpkab/1_4m5awnik" target="_blank">
      <img src="/assets/lectures/lecture4-thumb.jpg" alt="Lecture 4 thumbnail" style="width: 100%; border-radius: 8px;">
    </a>
    <div style="margin-top: 0.5em; font-size: 0.95em;"><span class="small-caps">lecture 4</span>: Practical <span class="small-caps">ai</span> for Imaging</div>
  </div>

  <div style="text-align: center; width: 220px;">
    <a href="https://mssvideoupload.mayo.edu/playlist/dedicated/1_uq8gpkab/1_h9wwrlo9" target="_blank">
      <img src="/assets/lectures/lecture5-thumb.jpg" alt="Lecture 5 thumbnail" style="width: 100%; border-radius: 8px;">
    </a>
    <div style="margin-top: 0.5em; font-size: 0.95em;"><span class="small-caps">lecture 5</span>: How Does Chat<span class="small-caps">gpt</span> Work?</div>
  </div>

  <div style="text-align: center; width: 220px;">
    <a href="https://mssvideoupload.mayo.edu/playlist/dedicated/1_uq8gpkab/1_euus6hqk" target="_blank">
      <img src="/assets/lectures/lecture6-thumb.jpg" alt="Lecture 6 thumbnail" style="width: 100%; border-radius: 8px;">
    </a>
    <div style="margin-top: 0.5em; font-size: 0.95em;"><span class="small-caps">lecture 6</span>: Prompting Chat<span class="small-caps">gpt</span></div>
  </div>

</div>


<div style="display:flex; align-items:flex-start; gap:1.2em; margin-bottom:1em;">
    <h2>2023</h2>
</div>

<div style="display: flex; align-items: flex-start; gap: 1.2em; margin-bottom: 0;">
  <h3 style="margin-bottom: 0.0em; margin-top: 0em;">Timewarp: Transferable Acceleration of Molecular Dynamics by Learning Time-Coarsened Dynamics</h3>
</div>
<div style="font-variant: small-caps; margin-top: -0.1em; margin-bottom: 0.7em;">
  online reading group presentation
</div>

Co-first author [Leon Klein](https://www.mi.fu-berlin.de/en/math/groups/ai4s/staff/klein/index.html) and I presented our <span class="small-caps">n</span>eur<span class="small-caps">ips</span> 2023 spotlight paper on using deep learning to accelerate molecular dynamics simulation at two online reading groups.

<div style="display: flex; justify-content: center; gap: 2em; flex-wrap: wrap; margin-bottom: 2em;">

  <!-- First Video -->
  <div style="flex: 1 1 45%; text-align: center;">
    <div style="aspect-ratio: 16 / 9;">
      <iframe 
        src="https://www.youtube.com/embed/4rtT-hE9Xqo?si=tfpzjzFw9n-gbMp_"
        title="YouTube video player"
        style="width: 100%; height: 100%; border: none;"
        allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
        referrerpolicy="strict-origin-when-cross-origin"
        allowfullscreen>
      </iframe>
    </div>
    <div style="font-style: italic; margin-top: 0.4em;">
      Learning on Graphs and Geometry <span class="small-caps">(l</span>o<span class="small-caps">gg)</span>
    </div>
  </div>

  <!-- Second Video -->
  <div style="flex: 1 1 45%; text-align: center;">
    <div style="aspect-ratio: 16 / 9;">
      <iframe 
        src="https://www.youtube.com/embed/fD_1V5HgGTQ"
        title="YouTube video player"
        style="width: 100%; height: 100%; border: none;"
        allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
        referrerpolicy="strict-origin-when-cross-origin"
        allowfullscreen>
      </iframe>
    </div>
    <div style="font-style: italic; margin-top: 0.4em;">
      Molecular Modeling and Drug Discovery <span class="small-caps">(m</span>2<span class="small-caps">d</span>2<span class="small-caps">)</span>
    </div>
  </div>

</div>


<div style="display:flex; align-items:flex-start; gap:1.2em; margin-bottom:1em;">
    <h2 style="margin:0;">2021</h2>
</div>

<div style="display: flex; align-items: flex-start; gap: 1.2em; margin-bottom: 0;">
  <h3 style="margin-bottom: 0.0em; margin-top: 0em;">An Introduction to PAC-Bayes</h3>
</div>

<div style="font-variant: small-caps; margin-top: -0.1em; margin-bottom: 0.7em;">
  cambridge machine learning reading group
</div>


I gave an introductory talk on the statistical learning framework and <span class="small-caps">pac</span>-Bayes with [David Burt](https://davidrburt.github.io/) and [Javier Antoran](https://javierantoran.github.io/about/). View the [slides](../files/pac_bayes_reading_group.pdf).

<div style="width: 95%; margin: 0 auto; text-align: center; margin-bottom: 1em;">
  <div style="aspect-ratio: 16 / 9;">
    <iframe 
      src="https://www.youtube.com/embed/t5GBuBD0ibc"
      title="YouTube video player"
      style="width: 100%; height: 100%; border: none;"
      allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
      referrerpolicy="strict-origin-when-cross-origin"
      allowfullscreen>
    </iframe>
  </div>
</div>

<div style="margin-bottom: 2em;"></div>


<div style="display:flex; align-items:flex-start; gap:1.2em; margin-bottom:0.2em;">
    <h3 style="margin-bottom:0; margin-top:1em;">
      Understanding Approximate Inference in Bayesian Neural Networks
    </h3>
</div>

<div style="font-variant: small-caps; margin-top: -0.1em; margin-bottom: 0.7em;">
  joint talk with oxford university 
</div>

I gave a talk on my <span class="small-caps">n</span>eur<span class="small-caps">ips</span> 2020 [paper](https://arxiv.org/abs/1909.00719) on approximate inference in Bayesian neural networks, with an accompanying talk by [Sebastian Farquhar](https://sebastianfarquhar.com/) of the University of Oxford. Our talks present different perspectives on the effectiveness of the mean-field approximation in these models. View the [slides](../files/BNNs_talk.pdf). 

<div style="width: 95%; margin: 0 auto; text-align: center; margin-bottom: 1em;">
  <div style="aspect-ratio: 16 / 9;">
    <iframe 
      src="https://www.youtube.com/embed/BJTkLxSQrHI"
      title="YouTube video player"
      style="width: 100%; height: 100%; border: none;"
      allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
      referrerpolicy="strict-origin-when-cross-origin"
      allowfullscreen>
    </iframe>
  </div>
</div>



<div style="display:flex; align-items:flex-start; gap:1.2em; margin-bottom:1em;">
    <h2 style="margin-bottom:0">2020</h2>
</div>

<div style="display: flex; align-items: flex-start; gap: 1.2em; margin-bottom: 0;">
  <h3 style="margin-bottom: 0.0em; margin-top: 0em;">On the Expressiveness of Approximate Inference in Bayesian Neural Networks</h3>
</div>

<div style="font-variant: small-caps; margin-top: -0.1em; margin-bottom: 0.7em;">
  presentation at neurips conference
</div>

A short video describing my paper on flaws in uncertainty estimation when using common approximate Bayesian neural network inference methods.
<!-- [SlidesLive link](https://slideslive.com/38937338/on-the-expressiveness-of-approximate-inference-in-bayesian-neural-networks?ref=speaker-44972-latest). -->

<div style="width: 95%; margin-top: 0; text-align: center; margin-bottom: 1em;">
  <div id="presentation-embed-38937338" style="width: 100%;"></div>

  <script src="https://slideslive.com/embed_presentation.js"></script>
  <script>
    embed = new SlidesLiveEmbed("presentation-embed-38937338", {
      presentationId: "38937338",
      autoPlay: false,
      verticalEnabled: false,
    });
  </script>
</div>


<div style="display:flex; align-items:flex-start; gap:0em; margin-bottom:0.2em;">
    <h3 style="margin-bottom:0; margin-top:1.7em;">
      Meta-learning Stationary Stochastic Process Prediction with Convolutional Neural Processes
    </h3>
</div>

<div style="font-variant: small-caps; margin-top: -0.1em; margin-bottom: 0.7em;">
  presentation at neurips conference
</div>


A short video explaining my paper on neural processes, a deep learning alternative to Gaussian processes for regression problems with uncertainty.
<!-- [SlidesLive link](https://slideslive.com/38937329/metalearning-stationary-stochastic-process-prediction-with-convolutional-neural-processes?ref=speaker-44972-latest). -->
<div id="presentation-embed-38937329"></div>
<script src="https://slideslive.com/embed_presentation.js"></script>
<script>
  embed = new SlidesLiveEmbed("presentation-embed-38937329", {
    presentationId: "38937329",
    autoPlay: false,
    verticalEnabled: false,
  });
</script>

<div style="display:flex; align-items:flex-start; gap:0em; margin-bottom:0.2em;">
    <h3 style="margin-bottom:0; margin-top:2.5em;">
      Neural Processes
    </h3>
</div>

<div style="font-variant: small-caps; margin-top: -0.1em; margin-bottom: 0.7em;">
  cambridge machine learning reading group
</div>

A reading group talk given with [Sebastian Ober](https://twitter.com/sebastian_ober?lang=en) and Stratis Markou, introducing various neural processes and covering much of the material in [this blog post](https://yanndubs.github.io/Neural-Process-Family/text/Intro.html). View the [slides](../files/nps_reading_group.pdf).



<div style="display:flex; align-items:flex-start; gap:0em; margin-bottom:0.2em;">
    <h3 style="margin-bottom:0; margin-top:1.7em;">
      Recent Advances in Bayesian Deep Learning
    </h3>
</div>

<div style="font-variant: small-caps; margin-top: -0.1em; margin-bottom: 0.7em;">
  cambridge machine learning reading group
</div>


A reading group talk given with [Siddharth Swaroop](https://siddharthswaroop.github.io/), covering modern stochastic gradient Markov chain Monte Carlo <span class="small-caps">(sgmcmc)</span> and natural gradient variational inference methods for Bayesian deep learning. View the [slides](../files/Recent_Advances_in_Bayesian_Deep_Learning.pdf).



<div style="display:flex; align-items:flex-start; gap:1.2em; margin-bottom:1em;">
    <h2 style="margin-bottom:0">2019</h2>
</div>

<div style="display: flex; align-items: flex-start; gap: 1.2em; margin-bottom: 0;">
  <h3 style="margin-bottom: 0.0em; margin-top: 0em;"><span style="display: inline-block; margin-left: -0.5em;">“</span>In-Between” Uncertainty in Bayesian Neural Networks</h3>
</div>

<div style="font-variant: small-caps; margin-top: -0.1em; margin-bottom: 0.7em;">
  contributed talk, icml 2019 workshop on uncertainty in deep learning
</div>


A contributed talk explaining my [paper](https://arxiv.org/abs/1906.11537) on the lack of “in-between” uncertainty when using the mean-field approximation in Bayesian neural networks.
Watch the [video](https://www.facebook.com/icml.imls/videos/320132412242165/?t=1720) (beginning at 28:30), view the [slides](../files/ICML_2019_Workshop_Presentation.pdf).



<div style="display:flex; align-items:flex-start; gap:0em; margin-bottom:0.2em;">
    <h3 style="margin-bottom:0; margin-top:1.7em;">
      Implicit Variational Inference
    </h3>
</div>

<div style="font-variant: small-caps; margin-top: -0.1em; margin-bottom: 0.7em;">
  cambridge machine learning reading group
</div>

I gave a talk with [David Burt](https://davidrburt.github.io/) introducing implicit variational inference, a way to obtain very flexible approximate posterior distributions for Bayesian inference. View the [slides](../files/Implicit_Inference_RG_notes.pdf).


<a id="blog" style="display: block; position: relative; top: -58px; visibility: hidden;"></a>
<h1>Blog Posts</h1>

*Occasional writings and tutorials on machine learning.*

<!-- Blog 1: Neural Process Family -->
<div style="display:flex; align-items:flex-start; gap:0.8em; margin-bottom:2em;">

  <img src="/assets/images/blogs/npf-icon.png"
       alt="Neural Process Family icon"
       style="width:50px; height:50px; object-fit:cover; border-radius:6px; flex-shrink:0;">

  <div>
    <h3 style="margin-top: 0em; margin-bottom: 1em;">
      <a href="https://yanndubs.github.io/Neural-Process-Family/text/Intro.html" target="_blank" rel="noopener noreferrer">
        The Neural Process Family
      </a>
    </h3>

    <p style="margin-bottom:0; margin-top:-0.8em; font-size: 0.9rem">
      Yann Dubois, Jonathan Gordon, <strong>Andrew Y. K. Foong</strong>
    </p>

    <p style="margin-top:0.4em;">
      Deep learning shines when there's lots of data, but fails in low-data settings where uncertainty matters, commonly encountered in medical time-series. The <span class="small-caps">npf</span> is a family of models that tackles this by meta-learning a distribution over predictors, blending stochastic processes with neural networks. This site walks through the ideas, maths, and code from scratch.
    </p>
  </div>
</div>

<!-- Blog 2: Bayesian Posts -->
<div style="display:flex; align-items:flex-start; gap:0.8em; margin-bottom:2em;">

  <img src="/assets/images/blogs/bayesian-icon.png"
       alt="Bayesian blog icon"
       style="width:50px; height:50px; object-fit:cover; border-radius:6px; flex-shrink:0;">

  <div>
    <h3 style="margin-top: 0em; margin-bottom: 1em;">
      <a href="https://mlg.eng.cam.ac.uk/blog/2021/03/31/what-keeps-a-bayesian-awake-at-night-part-1.html" target="_blank" rel="noopener noreferrer">
        What Keeps a Bayesian Awake at Night?
      </a>
    </h3>

    <p style="margin-bottom:0; margin-top:-0.8em; font-size: 0.9rem">
      Wessel P. Bruinsma, <strong>Andrew Y. K. Foong</strong>, Richard E. Turner
    </p>

    <p style="margin-top:0.4em;">
      A two-part blog exploring the sunny arguments <span style="font-style: italic">for</span> Bayesian inference, and then, in part two, taking a critical perspective. Written by members of an avowedly Bayesian Cambridge research group, we look at where the theory breaks, the modelling compromises we make, and the computational hurdles that keep us up at night.
      <br>

      <a href="https://mlg.eng.cam.ac.uk/blog/2021/03/31/what-keeps-a-bayesian-awake-at-night-part-1.html" target="_blank" rel="noopener noreferrer">
      <span class="small-caps">part 1: day</span>
      </a>
      &nbsp;&nbsp;<strong>|</strong>&nbsp;&nbsp;
      <a href="https://mlg.eng.cam.ac.uk/blog/2021/03/31/what-keeps-a-bayesian-awake-at-night-part-2.html" target="_blank" rel="noopener noreferrer">
      <span class="small-caps">part 2: night</span>
      </a>


    </p>
  </div>
</div>



<a id="cv" style="display: block; position: relative; top: -58px; visibility: hidden;"></a>
<h1>Curriculum Vitae</h1>

<p>
  My academic <span class="small-caps">cv</span> is available as a <span class="small-caps">pdf</span> 
  <a href="/files/CV.pdf" target="_blank" rel="noopener noreferrer">here</a>.
  <br>
  <span style="font-style: italic; color: #666;">Last updated January 2025.</span>
</p>